---
title: "Speak less, say more. FWO frequency reduction article"
author:
  - name: Anthe Sevenants
    email: anthe.sevenants@kuleuven.be
    orcid: 0000-0002-5055-770X
    affiliations:
      - name: KU Leuven
  - name: Freek Van de Velde
    email: freek.vandevelde@kuleuven.be
    orcid: 0000-0003-3050-2207
    affiliations:
      - name: KU Leuven
  - name: Dirk Speelman
    email: dirk.speelman@kuleuven.be
    orcid: 0000-0003-1561-1851
    affiliations:
      - name: KU Leuven
  - name: Dirk Pijpops
    email: dirk.pijpops@uantwerpen.be
    orcid: 0000-0002-3820-8099
    affiliations:
      - name: Universiteit Antwerpen
format:
  html:
    toc: true
    css: style.css
  docx:
    toc: false
  latex:
    theme: default
    standalone: true
    cite-method: biblatex
    filters: []
    documentclass: article
    toc: false
    reference-location: document
    wrap: none
    hyperrefoptions:
      - hidelinks=true
      - colorlinks=false
    no-highlight: true
    variables:
      tight-list: false
editor: source
title-block-banner: true
bibliography: references.bib
toc: true
toc-depth: 4
toc-location: left
tbl-cap-location: bottom
fig-cap-location: bottom
number-sections: true
reference-location: margin
csl: chicago-author-date.csl
df-print: kable
abstract: |
  TODO
execute:
  echo: false
---

## Introduction {#sec-introduction}

One of the prime usage-based mechanisms of language change is the reducing effect [@bybee_usage_2006]. The reducing effect states that, as constructions are used often, their phonetic representations become more sparse.
This entails that high frequency constructions reduce at a faster rate [@bybee_phonology_2003], and as a result experience more severe reduction compared to low-frequency constructions. 

The process of reduction is assumed to be the result of two adjacent processes: temporal reduction and substantive reduction [@mowrey_articulatory_1987]. Temporal reduction entails the compression of several articulatory gestures[^gesture] into one (i.e. assimilation or cluster simplification), while substantive reduction entails the reduction in magnitude of an articulatory gesture (i.e. vowel reduction to schwa). The combined effect of these two processes is then known under the more general term "reduction". We assume both these effects when we further use the term "reduction".

[^gesture]: An articulatory gesture can be defined as a movement of an articulator with an observable effect [@browman_articulatory_2009].

### Empirical traces of reduction

The existence of the reducing effect has been demonstrated through several studies. One of the earliest examples is @fidelholtz_word_1975, who shows that frequency can be related to vowel reduction in several phonetic contexts in English. Another example is @pluymaekers_lexical_2005: they tested the reduction of affixes in Dutch and found that reduction was positively related to the frequency of the carrier word. Finally, @bybee_effect_1999 discuss how English *don't* is most likely to be reduced in constructions where *don't* is highly frequent (like *I don't know*). These three studies respectively show that reduction can apply to words, bound morphemes and whole phrases.

In addition, several other synchronic factors have been found to influence the rate of reduction. In her overview of reduction, @ernestus_acoustic_2014 mentions the phenomenon to be influenced by phonological context (e.g. assimilation to ease the overlap of articulatory gestures), speech rate (reducing the number of gestures to be able to speak faster) and predictability [reducing the number of gestures because they are predictable from the context, independent from frequency, @bell_predictability_2009]. While we agree that these factors are also important in "on-line" reduction, in this study, we focus frequency of reduction in a wider diachronic context.

### Unknowns in the cause of the reducing effect {#sec-underspecification}

While we know from corpus studies that reduction patterns can indeed be found in speech data, what we cannot glean from corpora is the causality of these patterns. As a corpus only shows the resulting *effect* of language use on a large scale, we cannot see the decisions on the level of the individual that caused there to be a reducing effect in the first place. In this section, we will discuss three unknowns in reduction theory, the specifics of which have remained opaque in corpus studies.

The first unknown pertains to the frequency difference between the different constructions that undergo reduction. We know from Bybee's theory that high-frequency constructions reduce differently from low-frequency constructions [@bybee_phonology_2003, 11]. This implies that a frequency difference between constructions is necessary, but it is unspecified how that frequency difference needs to be realised. This leads to our first research question:

1. What frequency distribution do constructions in language have to follow for the reducing effect to arise as expected?[^expected]

The second unknown revolves around an built-in link between frequency and the speaker's inclination to reduce. @bybee_phonology_2003 [11] states:

> If sound changes are the result of phonetic processes that apply in real time as words are used, then those words that are used more often have more opportunity to be affected by phonetic processes.

The expectation by Bybee is that frequency in itself should be enough for reduction to arise organically. This implies there is no need for an built-in link that causes speakers to reduce more in high-frequency constructions than in low-frequency constructions. As this premise has not been tested, it is more an assumption rather than a finding. Therefore, our second research question is as follows:

2. Does there need to be an built-in link between frequency and the speaker's inclination to reduce for the reducing effect to arise as expected?[^expected]

Finally, existing theory on reduction also does not interact with the prospect of possible ambiguity. Reduction is, in essence, the act of expressing the same content in a narrower acoustic representation. Gradually, however, that also means that these representations will start looking more alike. It is currently unclear whether that implies some ambiguity avoidance mechanism is required to make sure that communicative confusion is avoided. Our final research question, then, is as follows:

3. Does there need to be an ambiguity avoidance mechanism for the reducing effect to arise as expected?[^expected]

[^expected]: For the reducing effect to arise "as expected", we mean that it should display the same properties as those we find in corpus data. Please see @sec-expected-behaviour for an explicit definition of expected reduction behaviour.

It is impossible to change reality in order to trace any cascading effects in the long term in corpus data. To find the requirements of the reducing effect in spite of this, we turn to computer simulations. Computer simulations allow one to virtualise reality into a model in which virtual language users ("agents") communicate with each other on the basis of simple, local rules [@steels_modeling_2011; @dekker_conversational_2024]. The idea is that the interplay of their interactions leads to emergent linguistic behaviour, which in our case is the reducing effect. Our goal in this article is to investigate which specification of assumptions is needed for the language in our computer model to exhibit the reducing effect, displaying the same properties as those found in corpus data. Because we work with computer simulations, we can assume several outlooks on reality and the reducing effect, and try several assumptions in order to find the minimal requirements needed for the reducing effect to occur.

While the real world is far messier and noisier than the idealised world represented in our simulations, simulation results can nevertheless "suggest that such underlying principles may operate in the real world as well" [@stanford_revisiting_2013, 122]. In our specific case, a simulation might show that typical reducing effect behaviour does not arise out of the preconditions that are presupposed in the literature, or that additional preconditions are required. Such outcomes can be theoretically interesting, since they make the various aspects of reducing effect theory more explicit, and since they can function as a stepping stone for further experimental or empirical research.

## Model design

In this section, we will first discuss the core architecture of the simulation. This is the part of the simulation that will remain invariant across parameter permutations. Our core architecture is based on empirical research in the usage-based linguistics paradigm [as recommended by @loreto_theoretical_2010, 69]. Usage-based linguistics views language as a complex adaptive system, i.e. a system shaped through repeated use by language users [@beckner_language_2009]. This makes it a natural fit for our agent-based simulations, which operate on the same principle. After we have introduced all core mechanisms in our simulation, we will discuss the variable parts of our model and the initial implementation we chose. We implemented our model of the reducing effect in Python using the MESA library [@python-mesa-2020]. All source code is available online.[^github-link]

[^github-link]: https://github.com/AntheSevenants/fwo-frequency-reduction

### Formalisation of speech {#sec-speech-formalisation}

For our reducing effect simulation, we made the decision to model speech using vector representations. Such vector representations are popular in the field of machine learning, both to represent meaning [@mikolov_efficient_2013] and acoustic information [@baevski_wav2vec_2020]. Since this model pertains to oral communication, having vector representations comparable to those found in the Speech Recognition field is especially interesting given the fact that reduction can also happen on a tonal level, e.g. in Mandarin [@de_smet_entrenchment_2016]. Reduction on such levels cannot be encoded in a typical written form, but they can be using vectors if we assume that the vectors encode phonetic information on all levels; the vector dimensions can be thought of as representing different acoustic frequency ranges, with the vector values representing the energy in those ranges.

For the basis of our vectors, we made the deliberate decision not to use any data from actual natural languages, since we wanted our model to be maximally language agnostic. Instead, we opted to use randomly generated speech representations, generating a matrix of $V \times D$, with $V$ being the number of constructions[^ctx] in the vocabulary of the agents and $D$ being the number of dimensions. The values of the vectors are randomly generated natural numbers between 70 and 100. The vectors are generated in such a way that, at the starting point of the simulations, they sit far enough apart not to be confused with each other. Of course, once the vectors start reducing, this will change. An example of the vector representations for the constructions in our model is given in @tbl-model_design_speech_vector_examples. Note that the representations in agents' memories will have additional noise added to these vectors (see @sec-agent-memory).

[^ctx]: By 'constructions', we mean pairings of meaning and forms learnt by language users [@goldberg_constructions_2006, 6], as these can be thought of as the most general units within usage-based linguistics that are eligible for undergoing reduction. 

|  Construction  | Dim 1 | Dim 2 | Dim 3 | ... | Dim $D$ |
| -------------- | ----- | ----- | ----- | ----- | ----- |
| Construction 1 | 71    | 89    | 85    | ...   | 83    |
| Construction 2 | 87    | 72    | 70    | ...   | 98    |
| ... | | | | |
| Construction $V$ | 88    | 76    | 91    | ...   | 88    |

: Example base vector representations for the constructions in the simulation model. These are the "original" vector representations that will have noise added to them once the agents' memories are initialised. {#tbl-model_design_speech_vector_examples}

Note also that, since are interested in the reduction of the form of constructions, the meaning of constructions is not explicitly modelled in our simulations. Instead, we implicitly assume that all meanings remain constant across different exemplars of the same constructions. This drive towards simplicity is in accordance with best practices in agent-based modelling [@landsbergen_cultural_2009; @beuls_agent-based_2013; @pijpops_lectal_2022].

### Agent memory {#sec-agent-memory}

The agents in our model have a memory which can store multiple realisations for each construction, a so-called exemplar memory [from exemplar theory, @bybee_usage_2006]. This allows for variation among those constructions, e.g. different realisations of *yes*: *yeah*, *yep*, *yup*, *ye* ... The same realisation can be stored in the exemplar memory multiple times as the result of more frequent exposure. Therefore, the typical realisation of a specific construction can naturally evolve as new realisations are added, i.e. reduced realisations.

There are two possible implementations for agent memory. The first possibility is an implementation where memory slots are not tied to specific constructions. All constructions compete for the same memory slots, which means that the number of exemplars per construction can evolve during the course of the simulation. The second implementation is an implementation where memory slots are pre-allocated to specific constructions. This makes the number of exemplars for each construction fixed for an entire simulation run. Theoretically, it seems best to choose the first implementation: usage-based theories of language generally take a very broad view on memory, with memory conceptualised as a large space that is shared by all sounds or constructions at once [i.e. @pierrehumbert_exemplar_2001]. As exemplar memory is said to reflect the observed speaking frequencies ["probability matching" in @labov_principles_1994], distributions between constructions will arise naturally, so there should be no need to predetermine which how many exemplars each construction can have in memory. Unfortunately, it is computationally intractable to equip agents with memories tallying hundreds of thousands of exemplars, which forces us to use a modest memory size. The issue with this unavoidable memory size concession is that in low memory contexts, new variants of high frequency constructions push out exemplars of low frequency constructions too easily, leading to those low-frequency constructions developing very few, unreliable representations. To guarantee the cognitive plausibility of our model, we therefore chose implementation two. Even though we use only a limited memory size, low-frequency constructions are still guaranteed a share of the total memory size in this implementation.

In our model, each agent has a memory of $L$ exemplars. This memory can be thought of as a matrix of size $L \times D$. The $L$ available slots in memory are pre-assigned to specific constructions according to a frequency distribution, with a lower limit of 5. @fig-memory_distribution shows an example distribution with a memory of size 40, four constructions and a lower limit of 5 exemplars per construction, organised according to a Zipfian distribution [@zipf_psycho-biology_1965]: note how the number of exemplars in memory taper off as the constructions become less frequent. This does not entail that all exemplars belonging to a single construction are identical: more and less reduced vectors can co-exist together. An example of an agent's memory with different vector associations can be found in @tbl-model_design_agent_memory_example.

![An example of the distribution of four constructions in an agent memory of size 40 with a lower limit of 5, organised according to a Zipfian distribution. The first construction appears 19 times, the second construction ten times, the third construction six times and the fourth construction five times.](tikz/memory_distribution.svg){#fig-memory_distribution}

|  Exemplar  | Dim 1 | Dim 2 | Dim 3 | ... | Dim $D$ | Associated construction |
| ---------- | ----- | ----- | ----- | --- | ----- | ------ |
| Exemplar 1 | 31    | 42    | 100    | ...   | 15    | Construction 1 |
| ... | | | | |
| Exemplar 16 | 27    | 37    | 27    | ...   | 97    | Construction 2 |
| Exemplar 17 | 24    | 35    | 29    | ...   | 96    | Construction 2 |
| ... | | | | |
| Exemplar $L$ | 44    | 39    | 69    | ...   | 80    | Construction $V$ |

: Example agent memory consisting of vector representations belonging to different constructions. {#tbl-model_design_agent_memory_example}

At the model initialisation stage, we fill each of the pre-assigned memory slots with vector representations from @tbl-model_design_speech_vector_examples. In this model initialisation stage, vector representations are not copied to an agent's memory one-to-one. Instead, noise is added in order to account for the natural variation in different speakers' idiomatic patterns. This noise added is drawn from a normal distribution with $\mu = 5$ and $\sigma = 1$, after which each value in the resulting vector is rounded to the nearest natural number. Substantially larger noise patterns would cause the distinctions between the exemplars of different constructions to become lost. Noise is added to each exemplar individually. If an agent starts with five exemplars of specific constructions in their memory, all five will be slightly different.

Because an agent's memory is limited to size $L$, whenever a new exemplar needs to be stored, an older exemplar needs to be removed or "forgotten". We apply the logic that the oldest exemplar of the construction in question is replaced. An example of the forgetting mechanic for a given construction is given in @tbl-model_design_forgetting_example.

|  Exemplar  | Age in model iterations | 
| ---- | ----- |
| ~~Exemplar 1~~ | 5920 |
| Exemplar 2 | 4780 |
| Exemplar 3 | 3009 |
| Exemplar 4 | 1231 |
| Exemplar 5 | 895 |

: An example of how "forgetting" an exemplar of a specific construction works in the model. Exemplar 1 is the oldest exemplar for this construction, so it will be removed and make place for a new exemplar. {#tbl-model_design_forgetting_example}

### Language game and course of the simulation {#sec-language-game}

We mentioned that we estimate that the interaction between agents, given specific preconditions, is naturally conducive to reducing behaviour. In this section, we will explain this interactional behaviour of our virtual speakers and hearers in more detail.

At each step in the simulation, each agent in the simulation enters into a "conversation" at random with another randomly selected agent. The agent initiating the conversation functions as the speaker, the other agent functions as the hearer. In this conversation, speaker and hearer play a so-called "language game" [@smith_models_2014], the core of our simulation. The goal of the language game is for the speaker to communicate a specific construction to the hearer, which the hearer then has to understand successfully. An overview of the game can be found in @fig-model_design_language_game. The language game goes as follows:

**Speaker**

1. The speaker is assigned a construction to utter from the $V$ constructions available. The probability of each construction being assigned is determined by the chosen frequency distribution (see @sec-underspecification-filling).[^zipfian-note]
2. The speaker randomly chooses an exemplar vector representation to utter for that construction. This exemplar is retrieved from the agent's memory. This means that if a specific neighbourhood area in the vector space contains more exemplars, there is a higher probability of the chosen exemplar coming from that area.
3. With a probability of $p$, reduction is applied. A set reduction value $R$ is removed from all dimensions of the vector at once, with $R$ also being the floor value for any dimension of a vector. Without the floor value $R$, vectors would be able to reduce all the way to zero, which is an acoustic representation of silence. Of course, communication through silence is also possible, but this contextually derivative situation is beyond the scope of our model. A schematic representation of reduction is given in @fig-model_design_reduction.
4. The (potentially reduced) vector is communicated to the hearer.

**Hearer**

1. The hearer "hears" the vector communicated by the speaker.
2. The hearer interprets the vector representation of the speaker. They calculate the Manhattan distance (see @eq-manhattan_distance) between the spoken vector and their entire exemplar memory (normalised for the number of dimensions) and select all exemplars lower or equal to threshold $n$. The constructions associated with those exemplars are tallied, and the most frequent construction is accepted as the understood construction. In the case of a tie, communication fails and the game immediately ends. In the case that no exemplars were selected, communication fails the game immediately ends. 
3. If communication succeeds, the exemplar heard is saved in the hearer's memory and is associated with the understood construction.

[^zipfian-note]: Note that we do not explicitly model a world or ground. The extralinguistic world is assumed through the assumed frequency distribution. For our model, the addition of an outside world with events would not have any added value, which is why we implemented agents about the world in this way.

![A schematic overview of the language game played by the speaker and hearer.](tikz/language_game.svg){#fig-model_design_language_game}

![How reduction works in our simulation. In this case, $R = 5$. The left vector shows the original vector as retrieved from memory. The right shows the result of reduction; 5 has been subtracted from all dimensions, except for the last, since its value is already at our floor value $R$.](tikz/reduction.svg){#fig-model_design_reduction}

$$
d(X, Y) = \frac{\sum_{i=1}^{D} |x_i - y_i|}{D} \text{where } D = \text{ number of dimensions}
$$ {#eq-manhattan_distance}

Note that we deliberately left out any feedback mechanism from the language game, as we are looking for the absolute minimal specification that is conducive to the reducing effect. Since including feedback implies adding an extra assumption, our initial aim is to attempt to model the reducing effect without a feedback mechanism.

### Moving away from the unknowns {#sec-underspecification-filling}

We saw in @sec-underspecification that several aspects of reducing effect theory are unknown or underspecified. Through our computer simulations, we will attempt to move away from these unknowns and build a minimal set of assumptions that leads to the reducing effect with all its properties. Our strategy throughout is to start with minimal but motivated assumptions. If these prove to be insufficient, we will make the specification more complex. As a reminder, these are the three unknowns defined in @sec-underspecification:

1. What frequency difference is required between different constructions?
2. Does there need to be an built-in association between frequency and the speaker's inclination to reduce?
3. Does there need to be an ambiguity avoidance mechanism?

As for (1), the minimal, motivated assumption can be inferred easily from corpus data. Indeed, constructions language follow Zipf's Law [@zipf_psycho-biology_1965]. Zipf's Law dictates that many "units" in language (constructions, sounds ...) naturally occur according to a power law in which rank is inversely related to frequency. This means that the first item in a Zipfian distribution is twice as frequent as the second item, three times as frequent as the third item, and so on. In practice, this leads to an extremely unbalanced distribution with few highly frequent items, and a long tail of infrequent items. This uneven distribution is also called an "A-curve" by @kretzschmar_language_2015. For our simulation, the frequency distribution is effective in the construction choice step. In addition, the exemplars in an agent's memory will also follow the Zipfian distribution ["probability matching" in @labov_principles_1994].

For our implementation of Zipf's Law, we chose the standard Zipfian equation (shown in @eq-zipfian_distribution). While other iterations of the Zipfian equation, devised to improve alignment with empirical data, exist [i.e. the Zipf-Mandelbrot equation, @mandelbrot_information_1966], these improved alignments are not perfect either [@montemurro_beyond_2001]. Most notably for our use case, the more involved Zipfian distribution equations require more parameters, which makes our search for the smallest set of assumptions conducive to the reducing effect more difficult. The only parameter in the original Zipfian equation is $\alpha$, which we set to 1.1.[^alpha-choice]

[^alpha-choice]: Our choice for $\alpha$ stems from a comparison with Dutch spoken corpus data [@schuurman_cgn_2003]. Note that the choice of $\alpha$ is, to a certain extent, arbitrary, and we do not expect qualitatively different results with other plausible parameter values such as 0.9 or 1.

$$
\operatorname{freq}(x) = \frac{1}{x^\alpha}
$$ {#eq-zipfian_distribution}

As for (2), we suppose the most minimal option and initially assume there not to be an built-in relation between frequency and the speaker's inclination to reduce. This assumption can be defended theoretically, as the "Principle of Least Effort" [@zipf_human_1949], dictates that when possible, language users will attempt to conserve as much energy as possible when speaking. Initially, then, we assume that this principle holds universally, regardless of frequency.

As for (3), we also started minimal and assumed that no ambiguity avoidance mechanism was required. We have no theoretical grounds for this assumption, other than the fact that this is the most minimal assumption possible.

Note that there is no built-in assumption which dictates that frequent constructions should reduce faster and to a larger extent than less frequent constructions. Rather, this is the emergent behaviour that needs to occur naturally out of the interplay of the different assumptions.

## Evaluation

In this section, we will explain how we will analyse the simulation behaviour, and how we will evaluate the model as a whole, particularly how we test whether the minimal assumptions from @sec-underspecification-filling lead to the occurrence of the reducing effect as defined by @bybee_usage_2006.

### Expected behaviour {#sec-expected-behaviour}

We will consider a set of preconditions to be conducive to the reducing effect if the following is true:

{{< include snippets/expected-behaviour-conditions.qmd >}}

If the behaviour of a simulation corresponds with any of these three statements not being true, then we will say that that simulation does not produce behaviour sufficiently similar to the behaviour found in corpus studies. The assumptions of that simulation are then said not to be conducive to the reducing effect. 

If any specification of assumptions produces behaviour which fully falls within the expected behaviour detailed in the previous section, then we can say that that specification is conducive to the reducing effect. The next question is whether this set is the *minimal* specification required to produce the reducing effect.

In order to check whether a specification of assumptions is also the minimal specification, we will apply the principle of Occam's Razor [@blythe_s-curves_2012]. This principle dictates that the simplest explanation for a phenomenon is presumably the most likely one. Therefore, we will disable each precondition iteratively and check whether the simpler conditions can also produce the expected reducing effect behaviour. If a simpler specification produces equivalent behaviour, we have found a more minimal specification. If no simpler specification produces equivalent behaviour, our initial specification was already the minimal one.

### Parameters

In this section, we will run through the different parameters the model has and what values we chose for them. Parameters control specific aspects of our model, like how many agents inhabit our virtual world or how likely reduction is. An important limitation to our parameter settings is computational tractability. Because we work with vector representations, and interpretation hinges on vector computations on an agent's entire exemplar memory, large agent counts, high dimension counts, large vocabulary sizes and large memory sizes all cause the time it takes to run a single simulation to balloon, even on powerful server hardware. Therefore, we had to keep number of agents and constructions to a modest amount. This is standard practice in agent-based simulation [cf. @dale_understanding_2012, 5]. Review @tbl-parameters for an overview of our choices.

| Parameter | Explanation | Value |
| --- | --- | --- |
| $N$ | Number of agents | 25 |
| $V$ | Number of constructions | 100 |
| $D$ | Number of vector dimensions | 10 |
| $L$ | Number of exemplars in memory | 1000 |
| $R$ | Value subtracted from exemplar vector at reduction time. Also functions as threshold floor value. A vector dimension cannot reduce below this point. | 5 |
| $n$ | Neighbourhood size. Only exemplars within this range count towards the interpretation of a vector. | 5 |
| $p$ | Reduction probability at speech time | 0.5 |
: An overview of all parameters in the simulation and their values. {#tbl-parameters}

In the following sections, we will present the results of several different parameter combinations ("specifications") of our reducing effect model. Because our model is partially probabilistic, each specification is run 100 times. Any graph shown is an aggregate of the behaviour of those 100 runs. We make this aggregate by taking the mean across all runs of a specification.

## Results

In this section, we will go over the results produced by the model operationalised according to the requirements detailed in the previous sections. More specifically, we will check if the model behaviour adheres to the expected behaviour defined in @sec-expected-behaviour.

### Base model {#sec-base-model}

First, we tested the simulation model with all minimal assumptions from @sec-underspecification-filling. As a reminder, this minimal specification is as follows:

1. Zipfian frequency difference between constructions
2. No association between frequency and the speaker's inclination to reduce
3. No ambiguity avoidance mechanism

We will discuss the behaviour of the simulation by going over the "checklist" defined in @sec-expected-behaviour, viz.: 

{{< include snippets/expected-behaviour-conditions.qmd >}}

The formulas at the basis of all visualisations can be found in the Appendix.

With respect to (1), @fig-base-model-l1-per-construction shows the average L1 of each construction separately. Recall that we expect frequent constructions to reduce a great extent, and faster as well. We indeed see that, generally speaking, frequent constructions (at the left of the graph) have lower L1 values than less frequent constructions (at the right of the graph).

As for (2), we can see in @fig-base-model-l1-general that the average L1, i.e. the average energy in the acoustic representations of constructions (see @sec-speech-formalisation) of the constructicon of the agents drops sharply and then remains more or less constant. Therefore, we can say that, indeed, the system seems to have reorganised itself to be more efficient. Of course, given the non-zero probability of reduction (see @tbl-parameters), this is to be expected.

In @fig-base-model-half-life-per-construction, the "half life" period for each construction is given. We show how many iterations in the simulation it took for every acoustic representation to reach half of its original energy. We see that for more frequent constructions (at the left of the graph), this half time tends to be reached faster than for less frequent constructions (at the right of the graph). Most constructions, viz. those in the Zipfian tail, do not even show up in the graph, since they have not reached half their original energy yet.

Regarding (3), we refer to @fig-base-model-success. It shows both the micro-averaged and macro-averaged communicative success in the model. The micro-average refers to the total number of successes, and is influenced by Zipfian sampling. Because most of the speaking turns will concern the most frequent construction, the communicative success of that specific construction weighs significantly on the global, unadjusted success rate. The macro-average is an average across averages: each construction contributes equally towards the average, and therefore overrules Zipfian sampling. While the micro-average indicates communicative success in language use, the macro-average gives a better of idea of communicative success in the language system.

We see in @fig-base-model-success that overall, communicative success in the model remains above chance level, although it does decrease. As we mentioned, it is expected that there is some trade-off between sparsity and communicative success, and the model results arguably remain within this margin. However, the confusion matrix in @fig-base-model-matrix paints a different picture: for the top 35 most frequent constructions, there is a large discrepancy between what the speaker intended (y axis) and what the hearer heard (x axis). More specifically, several frequent constructions seem to become confused with the most frequent construction. If we take English words as an illustration, this behaviour is comparable to speakers saying words like "have", "you", or "but", with hearers always hearing the most frequent English word "the". This is a major flaw in the communication system of the agents, and it also does not correspond with realistic linguistic behaviour: reduction in real life does not lead to mass confusion. 

@fig-base-model-confusion-ratio shows the "correct interpretation ratio" for the ten most frequent constructions. The ratio expresses from 0 to 1 what share of exemplars, across agents, comes from successful interactions between agents. If an agent mistakes a heard form for a form that actually stands for another construction, the exemplar collection for the correct construction becomes diluted: another form is now also (wrongly) associated with that construction. If the exemplar collection for a specific construction contains a lot of mishearings, its correct interpretation ratio goes down. Here specifically, we see that the most frequent construction has a lower correct interpretation ratio than the other constructions. This makes sense, given how often it is confused with other constructions. The issue, however, is that we cannot know from these metrics exactly *how* the confusion arises and why the most frequent construction specifically becomes the target of the confusion. To find out the cause of confusion, we built another model, which we will discuss in the next section.

::: {#fig-base-model-group layout-ncol=2}

![Average L1 evolution in the base model (across constructions, across agents). The x axis shows how many iterations the simulations have run for.](figures/fig-base-model-l1-general.png){#fig-base-model-l1-general}

![Average L1 value in the base model (per construction, across agents, 50,000 iterations). The x axis shows each construction from most frequent to less frequent.](figures/fig-base-model-l1-per-construction.png){#fig-base-model-l1-per-construction}

![Average L1 half life in the base model (per construction, across agents, 50,000 iterations). The x axis shows each construction from most frequent to less frequent.](figures/fig-base-model-half-life-per-construction.png){#fig-base-model-half-life-per-construction}

![Global communicative success in the base model (across constructions, across agents). The micro-average is taken across all constructions. The macro-average is an average across averages, with each construction contributing equally towards the average. The x axis shows how many iterations the simulations have run for.](figures/fig-base-model-success.png){#fig-base-model-success}

![Confusion matrix showing the mistakes that agents make in the base model. The y axis shows the intended constructions, the x axis shows the understood construction (across agents). Both x and y axis show the top 35 most frequent constructions from most frequent to less frequent.](figures/fig-base-model-matrix.png){#fig-base-model-matrix}

![Correct interpretation ratio evolution in the base model (per construction, across agents). The ratio expresses how many times a specific construction was interpreted correctly. The x axis shows how many iterations the simulations have run for. 1-10 refer to the 10 most frequent constructions, with line colour indicating frequency (darker is more frequent).](figures/fig-base-model-confusion-ratio.png){#fig-base-model-confusion-ratio}

An overview of the different simulation behaviour metrics and their evolutions for the base model.
:::

### Cone model {#sec-cone-model}

The main reason why we cannot reliably investigate the cause of confusion in the base model stems from the high dimensionality of the vectors. Because we work in a 10-dimensional space, there is no straightforward way to see how the vectors evolve over time. Unfortunately, dimensionality reduction techniques obscure a large share of the variation in the vectors, and are therefore unreliable to estimate the actual course of the acoustic representations.

The solution seems simple: reducing the number of dimensions to just two, which allows you to plot the acoustic representations accurately and reliably. This is indeed the solution we opted for, but with a few adjustments. We chose to represent the acoustic space as a cone, because this brings the properties of a two-dimensional space closer to the behaviour of the multi-dimensional space: all representations have a straight path to the apex (there is no need to go "through" another representation), and the phonetic space becomes more sparse as one nears the apex. Other adjustments are mechanical adaptations to the conic shape: all exemplars now reduce by moving at a straight angle towards the apex, and all constructions start at an equal radius of 100 from the apex. We limit ourselves to just ten constructions for this demonstration.[^frequency-truncation]

[^frequency-truncation]: For the frequency distribution of these ten items, we use the first ten frequencies from the larger, 100-construction Zipfian distribution found in the base model.

@fig-cone-model-group shows the vector space both at the beginning of the simulation and after 700 iterations. @fig-cone-model-angle-vocabulary-plot-2d-begin shows that, at the start of the simulation, the exemplars of different constructions are well spaced out. However, as the acoustic representations become sparser (i.e. move closer towards point 0,0), the space *between* representations also becomes smaller. This has the side-effect that the proximity of the exemplars of more frequent constructions will start to disturb the interpretation process described in @sec-language-game. Because our interpretation system hinges on frequency, the presence of a more frequent construction in the neighbourhood will skew this system, with the more frequent exemplars dominating perception. Mishearings of the associated constructions are the consequence. As the acoustic space becomes sparser and exemplars move even closer towards each other, more frequent constructions will iteratively start to overtake less frequent constructions until the most frequent form dominates the entire acoustic space. We see this in @fig-cone-model-angle-vocabulary-plot-2d-end: where the second and third most frequent constructions (green and blue respectively) first outnumber other constructions in their proximity, they become outnumbered once their interpretation neighbourhood becomes influenced by the most frequent construction (red).

The reason why we do not see this iterative process reflected in the base model is again the high dimensionality of the vectors. In the cone model, there are only two dimensions in which constructions can inch closer together. In the 100-dimensional model, the most frequent construction can be the closest neighbours of multiple constructions at once. The mechanics behind the confusion, however, are the same. A possible solution for these mishearings is given in the next section.

::: {#fig-cone-model-group layout-ncol=2}
![A flattened representation of the exemplar locations of a random agent in the cone model (starting situation, 0 iterations). The x and y axes show the values of each representation directly. Each colour stands for a construction, with each triangle being an exemplar of a construction. All ten constructions are shown. The closer an exemplar is situated to the lower left corner, the more reduced it is. The interpretation neighbourhood is shown as a circle.](figures/fig-cone-model-angle-vocabulary-plot-2d-begin.png){#fig-cone-model-angle-vocabulary-plot-2d-begin}

![A flattened representation of the exemplar locations of a random agent in the cone model (700 iterations). The x and y axes show the values of each representation directly. Each colour stands for a construction, with each triangle being an exemplar of a construction. All ten constructions are shown. The closer an exemplar is situated to the lower left corner, the more reduced it is. The interpretation neighbourhood is shown as a circle.](figures/fig-cone-model-angle-vocabulary-plot-2d-end.png){#fig-cone-model-angle-vocabulary-plot-2d-end}

An overview of the exemplar vector locations and their evolutions for the cone model.
:::

### Re-entrance model {#sec-reentrance-model}

Now that we can see the mishearings happen in real time, the question arises how they can be avoided. A complicating factor is that we intentionally did not incorporate a feedback mechanism in our simulation: hearers cannot know that they are mistaking a construction for another, so they cannot say "come again?". Vice versa, a speaker cannot know whether a reduced form will cause a mishearing on behalf of the hearer, as they do not have access to the acoustic representations of the latter. As such, our model design without any feedback mechanism seems to lead to a linguistic catch 22.

Nevertheless, a solution within this design is possible on behalf of the speaker. Since language production is considered a complex, automatic and unconscious process, its output cannot always be guaranteed to lead to communicative success. It is in this light that @steels_language_2003 has coined the term "re-entrance". Re-entrance can be understood as the application of a speaker's reception subsystem to their own linguistic utterances, to serve as a means of incomprehension prevention or repair. The concept of self-monitoring is also well established in psycholinguistics is part of production models [@hartsuiker_error_2001]. Psycholinguistic experiments such as @ferreira_how_2005 show empirical evidence for "comprehension monitoring", indicating that speakers can monitor their articulatory loop and preemptively avoid errors and ambiguities. Re-entrance is mechanically straightforward: a speaker's intended production is routed through their own reception mechanism, and its adequacy is subsequently assessed. It is in exactly this way that we have implemented this mechanism in our simulation. If a speaker reduces an acoustic representation, we first let them reflect on whether they would understand the uttered form correctly themselves. If they would, communication can continue as planned. If they would not, reduction is reversed, and the unreduced form is communicated.

![A schematic overview of the language game played by the speaker and hearer, with re-entrance. Before speaking, the speaker checks whether they would understand the uttered form themselves. If they do, reduction is kept, but if they do not, reduction is reversed. The new elements in the overview are indicated in teal.](tikz/language_game_reentrance.svg){#fig-model_design_language_game_reentrance}

With this new precondition added to our basic assumptions, the model now behaves as expected. @fig-reentrance-model-l1-general shows an increase in sparsity, @fig-reentrance-model-l1-per-construction and @fig-reentrance-model-half-life-per-construction show reduction extent and speed to be influenced by frequency, @fig-reentrance-model-success shows adequate communicative success and @fig-reentrance-model-matrix and @fig-reentrance-model-confusion-ratio show little to no confusion. Therefore, it seems that an ambiguity avoidance mechanism is indeed required to obtain simulations results comparable with corpus data, leading to the following specification:

1. Zipfian frequency difference between constructions
2. No association between frequency and the speaker's inclination to reduce
3. Re-entrance as an ambiguity avoidance mechanism

::: {#fig-reentrance-model-group layout-ncol=2}

![Average L1 evolution in the re-entrance model (across constructions, across agents). The x axis shows how many iterations the simulations have run for.](figures/fig-reentrance-model-l1-general.png){#fig-reentrance-model-l1-general}

![Average L1 value in the re-entrance model (per construction, across agents, 50,000 iterations). The x axis shows each construction from most frequent to less frequent.](figures/fig-reentrance-model-l1-per-construction.png){#fig-reentrance-model-l1-per-construction}

![Average L1 half life in the re-entrance model (per construction, across agents, 50,000 iterations). The x axis shows each construction from most frequent to less frequent.](figures/fig-reentrance-model-half-life-per-construction.png){#fig-reentrance-model-half-life-per-construction}

![Global communicative success in the re-entrance model (across constructions, across agents). The micro-average is taken across all constructions. The macro-average is an average across averages, with each construction contributing equally towards the average. The x axis shows how many iterations the simulations have run for.](figures/fig-reentrance-model-success.png){#fig-reentrance-model-success}

![Confusion matrix showing the mistakes that agents make in the re-entrance model. The x axis shows the intended constructions, the y axis shows the understood construction (across agents). Both x and y axis show the top 35 most frequent constructions from most frequent to less frequent.](figures/fig-reentrance-model-matrix.png){#fig-reentrance-model-matrix}

![Correct interpretation ratio evolution in the re-entrance model (per construction, across agents). The ratio expresses how many times a specific construction was interpreted correctly. The x axis shows how many iterations the simulations have run for. 1-10 refer to the 10 most frequent constructions, with line colour indicating frequency (darker is more frequent).](figures/fig-reentrance-model-confusion-ratio.png){#fig-reentrance-model-confusion-ratio}

An overview of the different simulation behaviour metrics and their evolutions for the re-entrance model.
:::

### Evaluation

We explained in @sec-expected-behaviour that the key point of simulation research is the search for the minimal specification of conditions that produces a specific effect. This is our current minimal specification which is conducive to the reducing effect: 

1. Zipfian frequency difference between constructions
2. No association between frequency and the speaker's inclination to reduce
3. Re-entrance as an ambiguity avoidance mechanism

We know that (3) is essential for reducing effect behaviour, as the results for our base model (@sec-base-model) did not satisfy our expected behaviour. As for (2), it never proved necessary in the (successful) re-entrance model (@sec-reentrance-model). This leaves us with our Zipfian assumption in (1). Is it really needed to lead to the reducing effect?

To investigate this, we built a model in which we sample constructions from a completely flat distribution instead of a Zipfian one. This means that all constructions have the same probability of being selected, and they also all claim the same number of slots in agent memory. The difference between the two distributions is demonstrated in @fig-evaluation-probabilities, where we show the probabilities of each construction being chosen in the two distributions. The question is to what extent this change from Zipfian to flat frequency has an influence on the course of the simulation. We will address this in the next subsection.

![A comparison between a Zipfian distribution (in blue) and a flat distribution (in orange), expressed in the probabilities of constructions being chosen. The x axis shows the rank of the construction, the y axis shows its probability of being chosen (between 0 and 1). There are 100 constructions in total.](figures/fig-evaluation-probabilities.png){#fig-evaluation-probabilities}

#### No Zipfian distribution, no feedback {#sec-no-zipfian-model}

In this subsection, we look at the results of the model in which the Zipfian distribution was replaced with a flat distribution. We will evaluate the same dimensions as in the previous model specifications (see @sec-expected-behaviour):

@fig-no-zipfian-model-l1-general shows that for this flat distribution model, the sparsity of the system still increases. However, when we look at @fig-no-zipfian-model-l1-per-construction and @fig-no-zipfian-model-half-life-per-construction, we do not see the same differentiation in reduction extent and speed under the influence of frequency. In addition, from @fig-no-zipfian-model-success, we can tell that this model delivers poor communicative performance: communicative success is low.[^confusion-matrix-missing] It appears that skewed distributions seem relevant for maintaining communicative stability during reduction. The question rises, however, how and why this is the case.

[^confusion-matrix-missing]: We do not show the confusion matrix here, as the essentially random variation gets obscured through the pooling operation across iterations.

To try and explain this behaviour, we take two approaches. In the first approach, we build feedback into the flat distribution model. In the second approach, we increasingly skew an exponential frequency distribution to check at what point communicative success matches the model with the Zipfian distribution without feedback. We will go over these two approaches in the following subsections.

::: {#fig-no-zipfian-model-group layout-ncol=2}

![Average L1 evolution in the re-entrance model (across constructions, across agents). The x axis shows how many iterations the simulations have run for.](figures/fig-no-zipfian-model-l1-general.png){#fig-no-zipfian-model-l1-general}

![Average L1 value in the flat distribution model (per construction, across agents, 50,000 iterations). The x axis shows each construction from most frequent to less frequent.](figures/fig-no-zipfian-model-l1-per-construction.png){#fig-no-zipfian-model-l1-per-construction}

![Average L1 half life in the flat distribution model (per construction, across agents, 50,000 iterations). The x axis shows each construction from most frequent to less frequent.](figures/fig-no-zipfian-model-half-life-per-construction.png){#fig-no-zipfian-model-half-life-per-construction}

![Global communicative success in the flat distribution model (across constructions, across agents). The micro-average is taken across all constructions. The macro-average is an average across averages, with each construction contributing equally towards the average. The x axis shows how many iterations the simulations have run for.](figures/fig-no-zipfian-model-success.png){#fig-no-zipfian-model-success}

An overview of the different simulation behaviour metrics and their evolutions for the flat distribution model.
:::

#### No Zipfian distribution, feedback {#sec-no-zipfian-feedback-model}

In this subsection, we investigate the model behaviour of the following specification:

1. No frequency difference between constructions
2. No association between frequency and the speaker's inclination to reduce
3. Re-entrance as an ambiguity avoidance mechanism
4. Feedback allowed during communication

While we originally opted not to use feedback in our language game because we did not want there to be some influence outside of language, perhaps some external, extralinguistic factor could be decisive in acting as a feedback mechanism. Therefore, as an exploratory measure, we additionally implement feedback for this specification. The addition of feedback adds another dimension to our model behaviour. At communication time, the hearer still uses their frequency-based approach to interpret the heard construction, but they now also receive a feedback signal from the speaker. If their interpretation was correct, they save the exemplar heard in their agent memory. If not, they do not save anything, and communication fails (i.e. there is no further repair mechanism). The revised flow of the language game is shown schematically in @fig-model_design_language_game_reentrance_feedback.

![A schematic overview of the language game played by the speaker and hearer, with re-entrance and feedback. After the exemplar is interpreted by the hearer, the speaker confirms whether this interpretation was correct. If so, the exemplar is saved, but if not, communication fails and nothing is saved. The new elements in the overview are indicated in teal.](tikz/language_game_reentrance_feedback.svg){#fig-model_design_language_game_reentrance_feedback}

When we look at the results, we see that the communicative chaos from the no feedback model has completely disappeared (@fig-no-zipfian-model-feedback-success and @fig-no-zipfian-model-feedback-matrix). Communication now proceeds without issue. Still, there is no differentiation in reduction extent and speed under the influence of frequency (@fig-no-zipfian-model-feedback-l1-per-construction and @fig-no-zipfian-model-feedback-half-life-per-construction).

::: {#fig-no-zipfian-model-feedback-group layout-ncol=2}

![Average L1 value in the flat distribution model with feedback (per construction, across agents, 50,000 iterations). The x axis shows each construction from most frequent to less frequent.](figures/fig-no-zipfian-model-feedback-l1-per-construction.png){#fig-no-zipfian-model-feedback-l1-per-construction}

![Average L1 half life in the flat distribution model with feedback (per construction, across agents, 50,000 iterations). The x axis shows each construction from most frequent to less frequent.](figures/fig-no-zipfian-model-feedback-half-life-per-construction.png){#fig-no-zipfian-model-feedback-half-life-per-construction}

![Global communicative success in the flat distribution model with feedback (across constructions, across agents). The micro-average is taken across all constructions. The macro-average is an average across averages, with each construction contributing equally towards the average. The x axis shows how many iterations the simulations have run for.](figures/fig-no-zipfian-model-feedback-success.png){#fig-no-zipfian-model-feedback-success}

![Confusion matrix showing the mistakes that agents make in the flat distribution model with feedback. The x axis shows the intended constructions, the y axis shows the understood construction (across agents). Both x and y axis show the top 35 most frequent constructions from most frequent to less frequent.](figures/fig-no-zipfian-model-feedback-matrix.png){#fig-no-zipfian-model-feedback-matrix}

An overview of the different simulation behaviour metrics and their evolutions for the flat distribution model with feedback.
:::

#### Increasingly skewed exponential distribution {#sec-exponential-model}

In the previous section, we saw that we can prevent communicative collapse in a flat distribution model if we incorporate a feedback mechanism. However, we saw in @sec-reentrance-model that with a Zipfian distribution, such a feedback mechanism is not necessary. To confirm this pattern, we investigate the link between a skewed (exponential) distribution and communicative success. We use the following specification:

1. Exponential frequency difference between constructions (to a varying degree)
2. No association between frequency and the speaker's inclination to reduce
3. Re-entrance as an ambiguity avoidance mechanism
4. No feedback allowed during communication

Our exponential distribution is organised according to @eq-exponential_distribution. We vary the skew in the exponential distribution from $\lambda = log_{10}\left(1\right)$ to $\lambda = log_{10}\left(3\right)$. An example of some of the resulting distributions can be found in @fig-exponential-probabilities.

$$
\operatorname{freq}(x) = e^{-\lambda \cdot x}
$$ {#eq-exponential_distribution}

![A comparison of different lambda values for the exponential distribution. We show $\lambda = log_{10}\left(1\right)$, $\lambda = log_{10}\left(2\right)$ and $\lambda = log_{10}\left(3\right)$. The x axis shows the rank of the construction, the y axis shows its probability of being chosen (between 0 and 1). There are 100 constructions in total. As lambda increases, so does the asymmetry of the probability distribution.](figures/fig-exponential-probabilities-2.png){#fig-exponential-probabilities}

Because this set of models covers a wide array of models, we built an aggregation graph over the model outcomes. @fig-no-zipfian-model-aggregate-success shows the average communicative success at the end of the model runs for each lambda parameter. It is clear that the larger lambda and thus the more skewed the distribution becomes, the better the model works. Broadly speaking, the more the distribution starts to resemble a typical Zipfian distribution, the better the model performs communicatively. We turn to the discussion to attempt to explain this behaviour.

![Global communicative success in the exponential distribution model (across constructions, across agents). This micro-average is taken across all constructions. The x axis shows the parameter value for lambda.](figures/fig-no-zipfian-model-success-graph.png){#fig-no-zipfian-model-aggregate-success}

## Discussion {#sec-discussion}

In this article, we attempted to find the minimal conditions necessary to produce the reducing effect as described by @bybee_usage_2006. The required behaviour for a successful model of reduction was as follows:

{{< include snippets/expected-behaviour-conditions.qmd >}}

We found that at least the following specification of specifications was required to lead to the reducing effect:

1. Zipfian frequency difference between constructions
2. No association between frequency and the speaker's inclination to reduce
3. Re-entrance as an ambiguity avoidance mechanism

If we replace the Zipfian distribution with a flat distribution, we also need an additional feedback mechanism to guarantee communicative stability. In this discussion section, we will go over the role of the Zipfian distribution, and what the necessity of the several requirements means for reducing effect theory and usage-based linguistics as a whole.

### Role of the Zipfian distribution

We saw that, paired with re-entrance, the Zipfian distribution can lead to a commmunicatively stable model that produces behaviour mimicking corpus data (@sec-reentrance-model). However, when we removed the Zipfian distribution and replaced it with a flat distribution, communication failed (@sec-no-zipfian-model). If we added feedback to the same specification, communication recovered (@sec-no-zipfian-feedback-model). Subsequently, we saw that feedback can be disabled and still lead to good results, as long as the distribution is decently skewed, i.e. comparable to a Zipfian distribution (@sec-exponential-model). In short, there seem to be two ways to achieve communicative success: either any distribution paired with feedback, or a heavily skewed distribution, like the Zipfian distribution, but without the requirement for feedback.

The major difference between the two strategies seems to lie in the way the evolution of representations is handled. If the relative frequency differences between the different constructions are relatively small, it becomes more difficult for a shared consensus to emerge regarding which constructions will undergo reduction first, second, third, etc. If each agent happens to, subconsciously, assign specific, more efficient representations to different constructions, communication is sure to go wrong. Over time, these subtle differences, amplified by mishearings or misinterpretations, can cause speakers to develop their own idiolects, i.e. i.e. their own distribution of the acoustic space, separate from other agents.

However, if constructions are organised according to a skewed frequency distribution, the likelihood of shared reduction patterns increases. As speakers will tend to use the same high-frequency constructions more often, they have more opportunities for those constructions to undergo similar processes of reduction. In this way, the frequency distribution itself subtly guides the alignment of linguistic behavior, making it easier for a collective pattern to emerge, even in the absence of direct feedback.

Contrastively, the implementation of a feedback mechanism allows agents to "synchronise" their reduction paths, even in challenging situations. Even if agents attempt to reduce constructions in disjoint orders, the feedback mechanism avoids mishearings and prevents idiolects from forming spontaneously. This is why a flat distribution also works if feedback is enabled: there is another mechanism, separate from the frequency distribution, which is able to force the "invisible hand" of the reduction process into a certain direction.

Note, however, that we do not assume the reducing effect to be *the* reason a Zipfian distribution is commonplace in language, as Zipfian distributions can be found in many aspects of reality [e.g. city size, @aitchison_zipfs_2016]. Regardless of their origin, however, Zipfian distributions can still be beneficial in the evolution of complex systems, like we see in our simulations. It is also not hard to imagine that a system that also works without an explicit feedback mechanism could be beneficial in situations where giving or receiving feedback is difficult, because of cognitive, ecological or other issues.

This ambiguity does make it hard to assess whether the Zipfian distribution is really a "required" assumption for the reducing effect. In a strict sense, it is not, since in the right circumstances with feedback, other distributions seem to work as well. Realistically, of course, the Zipfian frequency distribution seems to work in non-feedback circumstances as well, which makes it the most generally applicable specification. In light of the principle of Occam's Razor, the specification featuring a Zipfian distribution, but no feedback, is preferable (see @sec-expected-behaviour). In addition, the flat distribution with feedback still does not lead to a difference in reduction strength or speed, and thus does not attain all requirements outlined in @sec-expected-behaviour.[^transition] Other distributions lead to reduction, but no to the *reducing effect*.

[^transition]: Slightly skewed distributions can cause slight differences in reduction strength and speed because of their slight assymmetry. Still, they do not attain adequate communicative success.

### Implications for reducing effect theory

In our simulations, we found that at least three assumptions work together to ensure a communicatively stable system during the reduction process. The Zipfian distribution of construction frequency and with the continuous drive to spontaneously reduce forms are the driving forces behind the reducing effect proper. Additionally, self-understanding on behalf of speakers ("re-entrance") acts as a safeguard and keeps the virtual linguistic system from derailing. A skewed distribution is needed to keep the reduction paths equal without feedback. These are all aspects that have currently remained underspecified in reducing efect theory.

A detail of our implementation that has not been mentioned yet, but that also seems important for the reducing effect theoretically, is that of acoustic-perceptual resolution and its consequences for the avoidance of a fractal organisation of acoustic representations. Perception in our model is based on spatial vicinity. All exemplars within a specified range contribute towards the understanding of a particular form, and this range is fixed. This has the consequence that there is no use in constructions reducing to infinitely smaller forms, as an agent's perceptual range remains the same. Should our model be updated to allow for reduction in increasingly smaller steps, representations could theoretically keep reducing ad infinitum while keeping their current distinctions ("fractal"). They would just appear at increasingly smaller scales, at least as long as agents are perceptually sensitive to these small differences. Humans are known not to be sensitive to minute differences in acoustic frequency. In fact, an entire rescaling of the human hearing range has been devised for speech recognition purposes ["Mel scale", @pedersen_mel_1965] in order to account for the insensitivity of the human ear to certain frequency distinctions, especially in the upper frequency range. We did not include the avoidance of this fractal property as a precondition for the reducing effect, as we considered it moreso a detail of our implementation. Still, we think perceptual resolution is theoretically interesting and could be further investigated.

### Implications for usage-based linguistics

One of the cruxes of usage-based theories of language is a relatively straightforward categorisation of exemplars. However, we can say that the categorisation of exemplars is shown to not be as straightforward in our simulation as is assumed theoretically. While the idea is that proximity to other exemplars should suffice for correctly categorising or understanding exemplars [i.e. @pierrehumbert_exemplar_2001], frequency effects can disrupt this process and can cause cascading confusion. Of course, our simulation does not feature any sort of context nor feedback mechanism, which could help mitigate this categorisation problem. Nevertheless, our simulations show that, at least theoretically, circumstances such as sparsity can be disruptive for frequency-sensitive, usage-based theories of language storage.

More generally, our findings show that the "theoretical water" in usage-based linguistics is deeper than one might think. While the reducing effect, on paper, seems quite straightforward, our simulations show that in practice, its inner workings might be more intricate. As corpus results do not show causation, it could be theoretically interesting to investigate more usage-based theories in linguistics using computer simulations.

### Further research

There are several ways in which our model could be extended. The most obvious extension would be the inclusion of context. Indeed, while it was the very goal of our simulation to distill only the most fundamental aspects of the reducing effect, in real life situations, context contributes to how far a specific construction's representation can be reduced. If a construction is predictable from either its linguistic or its (real-life) situational context, there is a lesser need to express it fully. In this regard, it would be appropriate to turn our reducing effect model into a model of *probability* rather than one of frequency (although both are, of course, closely related).

Furthermore, our model currently does not account for the co-existence of reduced and unreduced forms on a diachronic level. While our exemplar model allows for a wide range of synchronic variation, agents will forget older, in our case unreduced forms once they fill up their exemplar memories. This leads to the loss of *I don't know* in favour of *dunno* over time, while both forms can also just co-exist. Other possible extensions include allowing constructions to disappear or varying the 'reducibility' of certain constructions to account for the fact that not *all* high frequency words seem eligible for reduction.

Finally, it would be valuable to bring the results of this simulation study into the real world by means of an experimental study [i.e. like @kirby_cumulative_2008]. In this way, it can be investigated if and how the simulation study might be translated into the context of real people. In this regard, our findings *in silico* can serve as headlights to make the transition to research *in vivo* more straightforward.

## Appendix {#sec-appendix-metrics}

In order to keep track of the simulation course, we computed the following metrics at each step:

1. **Communicative success:**  
  Ratio expressing in how many speaking turns the hearer understood the speaker correctly.  
  $$
  \frac{\text{\# successful turns}}{\text{\# turns}}
  $$
4. **Average agent L1:**  
  Number expressing the average L1 norm of exemplars, across all agents.  
  $$
  \frac{1}{N} \sum_{i=1}^{N}{\left[ \frac{1}{L_i} \sum_{j=1}^{L_i}{ \left[ \frac{\sum_{k=1}^{D}{ \text{agent}_i.\text{memory}_{jk}}}{D} \right] } \right]}
  $$
5. **Average construction L1:**  
  Number expressing for each construction $c$ the average L1 norm of its exemplars, across all agents.
  $$  
  \frac{1}{N} \sum_{i=1}^{N}{ \left[ \frac{1}{L^c_i} \sum_{j=1}^{L^c_i}{ \left[ \frac{\sum_{k=1}^{D}{ \text{agent}_i.\text{memory}_{jk}}}{D} \right] } \right] }
  $$ with $L^c$ = # exemplars associated with construction $c$
6. **Confusion matrix:**  
  Confusion matrix tallying intended and understood constructions across all agents. Matrix of size $V \times V$.
10. **Communicative success (macro average):**  
  Ratio expressing in how many speaking turns the hearer understood the speaker correctly. Macro averaged to disable Zipfian skew.
  $$
  \frac{1}{V} \sum_{t=1}^{V}{\frac{\text{\# successful turns}}{\text{\# turns for construction } t}}
  $$
11. **Correct interpretation ratio:**  
  Ratio expressing for each construction $c$ how many exemplars come from successful speaking turns.  
  $$
  \frac{1}{N} \sum_{i=1}^{N}{ \left[ \frac{1}{L^c_i} \sum_{j=1}^{L^c_i}{ \operatorname{good origin}(L^c_i) } \right] }
  $$ with $L^c$ = # exemplars associated with construction $c$ and $\operatorname{good origin}(exemplar) = \begin{cases}
0 & \text{if from failed speaking turn} \\
1 & \text{if from successful speaking turn}
\end{cases}$
12. **Half life time:**  
  Number expressing for each construction $c$ the timestep at which half of its original energy was reached.  
  $$
  \operatorname{min}(t) : E^c[t] \leq \frac{E^c[0]}{2}
  $$ with $t$ = time step in the model and $E^c$ list of L1 values per time step in the model for construction $c$