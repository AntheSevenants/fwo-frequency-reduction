---
title: "FWO frequency reduction article"
author:
  - name: Anthe Sevenants
    email: anthe.sevenants@kuleuven.be
    orcid: 0000-0002-5055-770X
    affiliations:
      - name: KU Leuven
  - name: Freek Van de Velde
    email: freek.vandevelde@kuleuven.be
    orcid: 0000-0003-3050-2207
    affiliations:
      - name: KU Leuven
  - name: Dirk Speelman
    email: dirk.speelman@kuleuven.be
    orcid: 0000-0003-1561-1851
    affiliations:
      - name: KU Leuven
  - name: Dirk Pijpops
    email: dirk.pijpops@uantwerpen.be
    orcid: 0000-0002-3820-8099
    affiliations:
      - name: Universiteit Antwerpen
format:
  html:
    toc: true
  docx:
    toc: false
editor: source
title-block-banner: true
bibliography: references.bib
toc: true
toc-depth: 4
toc-location: left
tbl-cap-location: bottom
fig-cap-location: bottom
number-sections: true
reference-location: margin
csl: chicago-author-date.csl
df-print: kable
abstract: |
  TODO
execute:
  echo: false
---

## Introduction {#sec-introduction}

Many processes in language change are characterised by frequency effects. Because of the way in which certain constructions are used more often than others, complex processes such as grammaticalisation can arise [todo]. Another example of a frequency effect is the reducing effect [bybee], which dictates that as constructions are used often, their phonetic representations become more sparse. Since the frequency of a construction is directly related to its reduction, this implies that high frequency constructions experience more severe reduction, while low frequency constructions experience reduction to a lesser extent.

The reason for the reduction process is so-called "neuromotor automation", according to Bybee [todo]:

> todo Bybee quote

This process of neuromotor automation is assumed to be the result of two adjacent processes: temporal reduction and substantive reduction [Mowrey & Pagliuca + Pagliuca & Mowrey]. Temporal reduction entails the compression of several articulatory gestures[^gesture] into one, while substantive reduction entails the reduction in magnitude of an articulatory gesture. The combined effect of these two processes is known under the more general term "reduction".

[^gesture]: An articulatory gesture can be defined as a movement of an articulator with an observable effect [todo].

### Empirical traces of reduction

[empirical displays of reducing effect]

### "Cause" of reduction

While we know from corpus studies that reduction patterns can indeed be found in textual data, and are therefore likely to exist, what we cannot glean from corpora is the causality that led to these patterns. As a corpus only shows the resulting *effect* of language use on a large scale, we cannot see the decisions on the level of the individual that caused there to be a reduction effect in the first place. We do not know, should reality have looked different, whether the reduction effect would still exist, as we do not know the factors that make the reduction effect occur. In other words, while we see the reducing effect in corpus data, we do not really know what *causes* it.

To find the requirements of the reduction effect, we turn to computer simulations. Computer simulations allow one to virtualise our reality into a model in which virtual language users ("agents") communicate with each other on the basis of simple, local rules. The idea is that the interplay of their interactions leads to emergent linguistic behaviour, which in our case is the reduction effect. Our goal in this article is to investigate which set of requirements is needed for the agents in our computer model to exhibit reduction, exhibiting the same properties as those found in corpus data. Because we work with computer simulations, we can assume several outlooks on reality and the reduction effect, and try several assumptions in order to find the absolute minimum requirements needed for the reduction effect to occur.

## Requirements

We mentioned in the previous section that our goal for this study is to find the minimal requirements which produce the reducing effect as defined by Bybee [todo] (see @sec-introduction). We deduce these minimum requirements from what is currently known in usage-based linguistics, a linguistic paradigm which dictates that language is a complex-adaptive system, i.e. a system shaped through repeated use by different language users. This makes it a natural fit for our simulation experiments, which operate on the same principle.

[todo: ik ben niet blij met hoe deze requirements "random" lijken, ze komen uit het niets. je hebt ze nodig maar dat weet je nog niet op dit punt. hoe oplossen?]

The first requirement for the reducing effect is about the very basis of communication: shared code. Language users must refer to the same concepts using roughly the same constructions, lest they cannot communicate. The presence of a shared code does not inhibit the reorganisation of the linguistic system to be more sparse (i.e. reduction), but it is a prerequisite for that reorganisation nonetheless.

The second requirement for reduction is related to the memory that a language user should have. We know from constructions like *I don't know* eroding into forms like *dunno* that higher-order storage and representation of constructions is necessary. If constructions were stored in a compositional way (i.e. only existing as the combination of lower-order building blocks), there would be no way for the construction as a whole to reduce. This indicates that any model of reduction should feature a memory that is based on exemplar theory [todo]: a memory built up as a collection of possible realisations of different constructions. This allows for variation among those constructions, e.g. different realisations of *yes*: *yeah*, *yep*, *yup* ... [todo misschien nog iets over voordeel vanwege frequency effects]

The third requirement for reduction is rooted in Zipf's Law [todo]. Zipf's Law dictates that many "units" in language (words, constructions, sounds ...) naturally occur according to a power law in which rank is inversely related to frequency. This means that the first item in a Zipfian distribution is twice as frequent as the second item, which in turn is twice as frequent as the third item, and so on. In practice, this leads to an extremely unbalanced distribution with few highly frequent items, and a long tail of infrequent items. This uneven distribution is also called an "A-curve" by @todo-kretzschmar. Note that, since linguistic items occur in an A-curve, so should the items in a language user's exemplar memory [todo Kretzschmar hst 3, also "probability matching" from Labov].

The final requirement for reduction is that language users should have a tendency to reduce words. This tendency arises naturally out of the so-called "Principle of Least Effort" [todo zipf], which dictates that when possible, language users will attempt to conserve as much energy as possible when speaking. This requires speakers to make an estimation of how much they can compress their utterance not to impede comprehension. The balance between energy conservation and utterance comprehension is the driving dynamic behind reduction.

Note that there is no built-in requirement which dictates that frequent forms should reduce faster and to a larger extent than less frequent forms. Rather, this is the emergent behaviour that needs to occur naturally out of the interplay of the different requirements.

## Model design

## Results

## Discussion