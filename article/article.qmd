---
title: "Speak less, say more. FWO frequency reduction article"
author:
  - name: Anthe Sevenants
    email: anthe.sevenants@kuleuven.be
    orcid: 0000-0002-5055-770X
    affiliations:
      - name: KU Leuven
  - name: Freek Van de Velde
    email: freek.vandevelde@kuleuven.be
    orcid: 0000-0003-3050-2207
    affiliations:
      - name: KU Leuven
  - name: Dirk Speelman
    email: dirk.speelman@kuleuven.be
    orcid: 0000-0003-1561-1851
    affiliations:
      - name: KU Leuven
  - name: Dirk Pijpops
    email: dirk.pijpops@uantwerpen.be
    orcid: 0000-0002-3820-8099
    affiliations:
      - name: Universiteit Antwerpen
format:
  html:
    toc: true
    css: style.css
  docx:
    toc: false
filters:
  - tikz
editor: source
title-block-banner: true
bibliography: references.bib
toc: true
toc-depth: 4
toc-location: left
tbl-cap-location: bottom
fig-cap-location: bottom
number-sections: true
reference-location: margin
csl: chicago-author-date.csl
df-print: kable
abstract: |
  TODO
execute:
  echo: false
---

## Introduction {#sec-introduction}

One of the prime usage-based mechanisms of language change is the reduction effect [@bybee_usage_2006]. The reduction effect dictates that, as constructions are used often, their phonetic representations become more sparse.  This entails that high frequency constructions experience more severe reduction, while low frequency constructions experience reduction to a lesser extent. In addition, high frequency constructions are also said to undergo reduction at a faster rate [@bybee_phonology_2003]. The reason for the reduction process itself is so-called "neuromotor automation", according to Bybee @bybee_usage_2006 [5].

The process of neuromotor automation is assumed to be the result of two adjacent processes: temporal reduction and substantive reduction [@mowrey_articulatory_1987]. Temporal reduction entails the compression of several articulatory gestures[^gesture] into one (i.e. assimilation or cluster simplification), while substantive reduction entails the reduction in magnitude of an articulatory gesture (i.e. vowel reduction to schwa). The combined effect of these two processes is then known under the more general term "reduction". We assume both these effects when we further use the term "reduction".

[^gesture]: An articulatory gesture can be defined as a movement of an articulator with an observable effect [@browman_articulatory_2009].

### Empirical traces of reduction

The existence of the reducing effect has been demonstrated through several studies. One of the earliest examples is @fidelholtz_word_1975, who shows that frequency can be related to vowel reduction in several phonetic contexts in English. Another example is @pluymaekers_lexical_2005: they tested the reduction of affixes in Dutch and found that reduction was positively related to the frequency of the carrier word. Finally, @bybee_effect_1999 discuss how English *don't* is most likely to be reduced in constructions where *don't* is highly frequent (like *I don't know*). These three studies respectively show that reduction can apply to the lexicon, morphology, and constructions as a whole.

In addition, several other factors have been found to influence the rate of reduction. In her overview of reduction, @ernestus_acoustic_2014 mentions the phenomenon to be influenced by phonological context (e.g. assimilation to ease the overlap of articulatory gestures), speech rate (reducing the number of gestures to be able to speak faster) and predictability (reducing the number of gestures because they are predictable from the context). While we agree that these factors are also important, in this study, we focus on raw frequency only for the sake of simplicity: it is one of the most basic, inherent properties of natural language. Once we can assess the role of frequency in reduction, we can expand the model to include other factors (see @sec-discussion).

### Unknowns in the cause of reduction {#sec-underspecification}

While we know from corpus studies that reduction patterns can indeed be found in speech data, and are therefore likely to exist, what we cannot glean from corpora is the causality that led to these patterns. As a corpus only shows the resulting *effect* of language use on a large scale, we cannot see the decisions on the level of the individual that caused there to be a reduction effect in the first place. We do not know, should reality have looked different, whether the reduction effect would still exist, as we do not know the factors that make the reduction effect occur. In other words, while we see the reducing effect in corpus data, we do not really know what *causes* it. In this section, we will discuss three unknowns (TODO?) in reduction theory, the specifics of which have remained opaque in corpus studies.

The first unknown pertains to the frequency difference between the different constructions that undergo reduction. We know from Bybee's theory that high-frequency constructions reduce differently from low-frequency constructions [@bybee_phonology_2003, 11]. This implies that a frequency difference between constructions is necessary, but it is unspecified how that frequency difference needs to be realised. This leads to our first research question:

1. What frequency distribution do constructions in language have to follow for the reducing effect to arise as expected?[^expected]

The second unknown revolves around an explicit link between frequency and reduction willingness on behalf of the speaker. @bybee_phonology_2003 [11] states:

> If sound changes are the result of phonetic processes that apply in real time as words are used, then those words that are used more often have more opportunity to be affected by phonetic processes.

The expectation by Bybee is that frequency in itself should be enough for reduction to arise organically. This implies there is no need for an explicit link that causes speakers to reduce more in high-frequency constructions than in low-frequency constructions. As this premise has not been tested, it is more an assumption rather than a finding. Therefore, our second research question is as follows:

2. Does there need to be an explicit link between frequency and reduction willingness on behalf of the speaker for the reducing effect to arise as expected?[^expected]

Finally, existing theory on reduction also does not interact with the prospect of possible ambiguity. Reduction is, in essence, the act of expressing the same content in a narrower acoustic representation. Gradually, however, that also means that these representations will start looking alike more. It is currently unclear whether that implies some ambiguity avoidance mechanism is required to make sure that communicative confusion is avoided. Our final research question, then, is as follows:

3. Does there need to be an ambiguity avoidance mechanism for the reducing effect to arise as expected?[^expected]

[^expected]: For the reducing effect to arise "as expected", we mean that it should display the same properties as those we find in corpus data. Please see @sec-sec-evaluation for an explicit definition of expected reduction behaviour.

Empirically, it is impossible to change reality in the past in order to trace any cascading effects in the long term in corpus data. To find the requirements of the reduction effect in spite of this, we turn to computer simulations. Computer simulations allow one to virtualise reality into a model in which virtual language users ("agents") communicate with each other on the basis of simple, local rules. The idea is that the interplay of their interactions leads to emergent linguistic behaviour, which in our case is the reducing effect. Our goal in this article is to investigate which set of requirements is needed for the language in our computer model to exhibit the reducing effect, displaying the same properties as those found in corpus data. Because we work with computer simulations, we can assume several outlooks on reality and the reducing effect, and try several assumptions in order to find the minimal requirements needed for the reducing effect to occur.

While the real world is far messier and noisier than the idealised world represented in our simulations, simulation results can nevertheless "suggest that such underlying principles may operate in the real world as well" [@stanford_revisiting_2013, 122]. In our specific case, a simulation might show that typical reduction behaviour does not arise out of the preconditions that are presupposed in the literature, or that additional preconditions are required. Such outcomes can be theoretically interesting, since they make the various aspects of reduction theory more explicit, and since they can function as a stepping stone for further experimental or empirical research.

## Model design

In this section, we will first discuss the core architecture of the simulation. This is the part of the simulation that will remain invariant across parameter permutations. Our core architecture is based on empirical research in the usage-based linguistics paradigm [as recommended by @loreto_theoretical_2010, 69]. Usage-based linguistics views language as a complex adaptive system, i.e. a system shaped through repeated use by language users [@beckner_language_2009]. This makes it a natural fit for our agent-based simulations, which operate on the same principle. After we have introduced all core mechanisms in our simulation, we will discuss the variable parts of our model and the initial implementation we chose. 

### Formalisation of speech

For our reduction simulation, we made the decision to model speech using vector representations. Such vector representations are popular in the field of machine learning, both to represent meaning [@mikolov_efficient_2013] and acoustic information [@baevski_wav2vec_2020]. Since this model pertains to oral communication, having vector representations comparable to those found in the Speech Recognition field is especially interesting given the fact that reduction can also happen on a tonal level, e.g. in Mandarin [@de_smet_entrenchment_2016]. Such reduction effects cannot be encoded in a typical written form, but they can be using vectors if we assume that the vectors encode phonetic information on all levels.

For the basis of our vectors, we made the deliberate decision not to use any data from actual natural languages, since we wanted our model to be maximally language agnostic. Instead, we opted to use randomly generated speech representations, generating a matrix of $V \times M$, with $V$ being the number of constructions[^ctx] in the vocabulary of the agents and $M$ being the number of dimensions.[^parameters] The values of the vectors are randomly generated natural numbers between R and 100. An example of the vector representations for the constructions in our model is given in @tbl-model_design_speech_vector_examples. Note that the representations in agents' memories will have additional noise added to these vectors (see @sec-agent-memory).

[^ctx]: We made the decision to model constructions rather than individual words or sounds as constructions can be thought of as the most general units within usage-based linguistics that are eligible for undergoing reduction. 

|  Construction  | Dim 1 | Dim 2 | Dim 3 | ... | Dim $M$ |
| -------------- | ----- | ----- | ----- | ----- | ----- |
| Construction 1 | 35    | 75    | 85    | ...   | 15    |
| Construction 2 | 87    | 72    | 47    | ...   | 67    |
| ... | | | | |
| Construction $V$ | 65    | 45    | 57    | ...   | 88    |

: Example base vector representations for the constructions in the simulation model. {#tbl-model_design_speech_vector_examples}

[^parameters]: We will later discuss the exact parameters chosen.

Note that, for the sake of simplicity, the meaning of the constructions in our simulations is not explicitly modelled. Instead, we implicitly assume that all meanings remain constant across different exemplars of the same constructions.

### Agent memory {#sec-agent-memory}

The agents in our model have a memory which can store multiple realisations for each construction, a so-called exemplar memory [from "exemplar theory", @bybee_usage_2006]. This allows for variation among those constructions, e.g. different realisations of *yes*: *yeah*, *yep*, *yup* ... The same realisation can be stored in the exemplar memory multiple times as the result of more frequent exposure. Therefore, the typical realisation of a specific construction can naturally evolve as new realisations are added, i.e. reduced realisations.

In our model, each agent has a memory of $L$ exemplars. This memory can be thought of as a matrix of size $L \times M$, with a separate mapping of size $L$ which keeps track of which exemplar belongs to which construction. In our case, is it imperative that $L > V$, else there cannot be multiple exemplars of the same construction. An example of an agent's memory with different vector associations can be found in @tbl-model_design_agent_memory_example.

|  Exemplar  | Dim 1 | Dim 2 | Dim 3 | ... | Dim $M$ | Associated construction |
| ---------- | ----- | ----- | ----- | --- | ----- | ------ |
| Exemplar 1 | 31    | 42    | 100    | ...   | 15    | Construction 15 |
| Exemplar 2 | 27    | 37    | 27    | ...   | 97    | Construction 87 |
| Exemplar 3 | 24    | 35    | 29    | ...   | 96    | Construction 87 |
| ... | | | | |
| Exemplar $L$ | 44    | 39    | 69    | ...   | 80    | Construction $V$ |

: Example agent memory consisting of vector representations belonging to different constructions. {#tbl-model_design_agent_memory_example}

At the model initialisation stage, we seed each agent's exemplar memory with one vector representation from @tbl-model_design_speech_vector_examples for each construction, i.e. $V$ exemplars in total. We then further fill this memory following the chosen frequency distribution (see @sec-underspecification-filling): exemplars are added to various constructions according to the chosen frequency distribution until the number of exemplars reaches $L$. Note that the output distribution of exemplars in an agent's memory would be the natural outcome of communication anyway, as constructions are chosen using the same distribution. However, by pre-filling the memory, we keep the number of forms in memory constant throughout the simulation, which should guarantee consistent model behaviour.

In this model initialisation stage, vector representations are not copied to an agent's memory one-to-one. Instead, noise is added in order to account for the natural variation in different speakers' idiomatic patterns. This noise added is drawn from a normal distribution with $\mu = 5$ and $\sigma = 1$. Substantially larger noise patterns would cause the distinctions between the forms of different constructions to become lost. Noise is added to each exemplar individually. If an agent starts with five exemplars of specific constructions in their memory, all five will be slightly different.

Because an agent's memory is limited to size $L$, whenever a new form needs to be stored, an older form needs to be removed or "forgotten". We apply the logic that the oldest exemplar that is not the last exemplar left of a construction is deleted. In this way, we avoid situations where the only exemplar left of a construction is removed, which would cause issues with mutual comprehension later in the simulation. Of course, forgetting a specific construction is normal in real life, but we avoid this situation in the simulation in order to keep the simulation mechanics as straightforward as possible. An example of the forgetting mechanic is given in @tbl-model_design_forgetting_example.

|  Exemplar  | Age in model steps | Associated construction | Exemplars left for construction |
| ---- | -----        | -----                   | ----- |
| Exemplar 1 | 1575 | Construction 15 | 2 |
| ~~Exemplar 2~~ | 2789 | Construction 15 | 2 |
| Exemplar 3 | 3009 | Construction 87 | 1 |
| ... | | | | |
| Exemplar $L$ | | | Construction $V$ |

: An example of how "forgetting" an exemplar works in the model. Even though Exemplar 3 is the oldest exemplar in the memory, because it is the last exemplar associated with Construction 87, Exemplar 2 is removed instead. {#tbl-model_design_forgetting_example}

### Reduction {#sec-reduction}

At speech time (see @sec-language-game), the speaker agent has the opportunity to reduce the exemplar vector representation that they retrieved from memory. Our implementation of reduction is extremely simple: a set reduction value $R$ is removed from all dimensions of the vector at once, with $R$ also being the floor value for any dimension of a vector. Without the floor value $R$, vectors would be able to reduce all the way to zero, which is an acoustic representation of silence. Of course, communication through silence is also possible, but this contextually derivative situation is beyond the scope of our model. A schematic representation of reduction is given in @fig-model_design_reduction.

::: {#fig-model_design_reduction}
```{.tikz}
\usetikzlibrary{shapes.misc, positioning, decorations.pathreplacing}

\begin{tikzpicture}[
    box/.style={draw, minimum width=1cm, minimum height=1cm, font=\Large},
    redbox/.style={box, fill=red!30},
    brace/.style={decorate, decoration={brace, amplitude=5pt}},
    arrow/.style={->, ultra thick}
]

\node[box] (A) {35};
\node[box, right=0cm of A] (B) {37};
\node[box, right=0cm of B] (C) {75};
\node[box, right=0cm of C] (D) {85};
\node[box, right=0cm of D] (E) {62};
\node[redbox, right=0cm of E] (F) {15};

\node[box, right=2cm of F] (G) {20};
\node[box, right=0cm of G] (H) {22};
\node[box, right=0cm of H] (I) {60};
\node[box, right=0cm of I] (J) {70};
\node[box, right=0cm of J] (K) {47};
\node[redbox, right=0cm of K] (L) {15};

\draw[brace] (A.north west) -- (F.north east);
\draw[brace] (F.south east) -- (A.south west);
\draw[brace] (G.north west) -- (L.north east);
\draw[brace] (L.south east) -- (G.south west);

\draw[arrow] ([xshift=0.5cm]F.east) -- ([xshift=-0.5cm]G.west);

\end{tikzpicture}
```

How reduction works in our simulation. In this case, $R = 15$. The left vector shows the original vector as retrieved from memory. The right shows the result of reduction; 15 has been subtracted from all dimensions, except for the last, since its value is already at our floor value $R$.
:::

### Language game and course of the simulation {#sec-language-game}

We mentioned that we estimate that the interaction between agents, given specific preconditions, is naturally conducive to reducing behaviour. In this section, we will explain this interactional behaviour of our virtual speakers and hearers in more detail.

At each step in the simulation, each agent in the simulation enters into a "conversation" at random with another randomly selected agent. The agent initiating the conversation functions as the speaker, the other agent functions as the hearer. In this conversation, speaker and hearer play a so-called "language game" [@smith_models_2014], the core of our simulation. In a language game, a speaker and a hearer agent perform a joint task rooted in language, the execution of this task naturally exerting some influence on the way language is used or organised. In our simulation, the goal of the language game is for the speaker to communicate a specific construction to the hearer, which the hearer then has to understand successfully. The exact course of the language game is as follows:

**Speaker**

1. The speaker chooses a construction to utter from the $V$ constructions available. The probability of each construction is influenced by the chosen frequency distribution (see @sec-underspecification-filling).[^zipfian-note]
2. The speaker chooses an exemplar vector representation to utter for that construction. This exemplar is retrieved from the agent's memory. There is no mechanism influencing the probability of a specific exemplar being chosen. Of course, if a specific neighbourhood area is more dense, there is a higher probability of the chosen exemplar coming from that area.
3. With a probability of $p$, reduction is applied (see @sec-reduction).
4. The (potentially reduced) vector is communicated to the hearer.

**Hearer**

1. The hearer "hears" the vector communicated by the speaker.
2. The hearer interprets the vector representation of the speaker. They calculate the cosine distance (see Equation TODO) between the spoken vector and their entire exemplar memory, and select all exemplars lower or equal to threshold $n$. The constructions associated with those exemplars are tallied, and the most frequent construction is accepted as the understood construction. In the case of a tie, communication fails and no exemplars are saved. In the case that no exemplars were selected, communication fails and no exemplars are saved. 
3. The heard exemplar is saved in the hearer's memory and is associated with the understood construction. The speaker does not adapt their own memory to keep the model design as simple as possible [todo literatuur ding lezen].

[^zipfian-note]: Note that we do not explicitly model a world or ground. The extralinguistic world is assumed through the assumed frequency distribution. For our model, the addition of an outside world with events would not have any added value, which is why we implemented agents about the world in this way.

![A schematic overview of the language game played by the speaker and hearer. The diagram shows the different steps of the language game. For the speaker, this means: choosing a construction, choosing an associated exemplar, applying reduction, and speaking the resulting vector representation. For the hearer, this means: hearing the exemplar, interpreting it by tallying the exemplars in the neighbourhood of the communicated vector location, and then finally committing the exemplar to memory.](language_game.svg){#fig-model_design_language_game}

Note that we deliberately left out any feedback mechanism from the language game. We assume that our agents can only communicate through language. Our intention was to make reduction work without the hearer agent knowing whether what they understood was indeed the construction intended by the speaker. If we implemented a way to communicate "perfect" feedback from speaker to hearer, there would be no point for our agents to communicate through language. In this way, the task our agents face also becomes harder. They need to evolve the language system into a more sparse representation without any feedback of how effective that representation actually is.

### Moving away from the unknowns {#sec-underspecification-filling}

We saw in @sec-underspecification that several aspects of reduction theory are unknown or underspecified. Through our computer simulations, we will attempt to move away from these unknowns and build a minimal set of assumptions that leads to the reducing effect with all its properties. Our strategy throughout is to start with minimal but motivated assumptions. If these prove to be insufficient, we will make the specification more complex. As a reminder, these are the three unknowns defined in @sec-underspecification:

1. What frequency difference is required between different constructions?
2. Does there need to be an explicit association between frequency and reduction willingness on behalf of the speaker?
3. Does there need to be an ambiguity avoidance mechanism?

As for (1), the minimal, motivated assumption can be inferred easily from corpus data. Indeed, words, constructions and in fact most other units in language follow Zipf's Law [@zipf_psycho-biology_1965]. Zipf's Law dictates that many "units" in language (words, constructions, sounds ...) naturally occur according to a power law in which rank is inversely related to frequency. This means that the first item in a Zipfian distribution is twice as frequent as the second item, three times as frequent as the third item, and so on. In practice, this leads to an extremely unbalanced distribution with few highly frequent items, and a long tail of infrequent items. This uneven distribution is also called an "A-curve" by @kretzschmar_language_2015. For our simulation, this has the consequence that in the construction choice stage (Speaker step 1 in @sec-language-game), agents are more likely to talk about high-frequent constructions rather than low-frequent constructions. In addition, the exemplars in an agent's memory will also follow the Zipfian distribution ["probability matching" in @labov_principles_1994].

As for (2), we suppose the most minimal option and initially assume there not to be an explicit relation between frequency and willingness of reduction. This assumption can be defended theoretically, as the "Principle of Least Effort" [@zipf_human_1949], dictates that when possible, language users will attempt to conserve as much energy as possible when speaking. Initially, then, we assume that this principle holds universally, regardless of frequency.

As for (3), we also started minimal and assumed that no ambiguity avoidance mechanism was required. TODO (We have no ecological evidence for this assumption. Op één of andere manier uitleggen. Deens kan pas later komen)

Note that there is no built-in assumption which dictates that frequent forms should reduce faster and to a larger extent than less frequent forms. Rather, this is the emergent behaviour that needs to occur naturally out of the interplay of the different assumptions.

## Methodology

Because we use computer simulations, we apply a different methodology from traditional, empirical research. In this section, we will explain how we will analyse the simulation behaviour, and how we will evaluate the model as a whole, particularly how test that the preconditions outlined in @sec-preconditions are indeed the minimal set required for reduction as defined by @bybee_usage_2006 to occur.

### Simulation behaviour metrics

One of the cornerstone principles of our simulation is the complex adaptive essence of natural language. As the language system is used, it is incrementally shaped by interactions. Therefore, it is imperative that we can minutely track exactly how the language system changes in each time step of the simulation. We tracked measures such as communicative success, mean construction energy and confusion among forms. To see an overview of our metrics and their computations, see @sec-appendix-metrics.

### Expected behaviour {#sec-expected-behaviour}

We will consider a set of preconditions to produce the reducing effect if the following is true:

1. The language system has reorganised itself to express the same constructions using more efficient representations.
1. Frequent forms have reduced to a greater extent than non-frequent forms.
1. Communication between agents remains successful. If the language system has reorganised itself in a way that makes effective communication impossible, the simulation has failed to simulate realistic conditions. Nonetheless, some tradeoff between communicative effort and communicative success can be seen as normal.

If the behaviour of a simulation corresponds with any of these three statements not being true, then we will say that that simulation does not produce behaviour equivalent to the behaviour found in corpus studies. The preconditions of that simulation are then said not to be conducive to the reducing effect.

### Evaluation {#sec-evaluation}

If any set of preconditions produces behaviour which fully falls within the expected behaviour detailed in the previous section, then we can say that that set is conducive to the reducing effect. The next question is whether this set is the *minimal* set required to produce the reducing effect.

In order to check whether a set of preconditions is also the minimal set, we will apply the principle of Occam's Razor [@blythe_s-curves_2012]. This principle dictates that the simplest explanation for a phenomenon is presumptively the most likely one. Therefore, we will disable each precondition iteratively and check whether the simpler conditions can also produce the expected reduction behaviour. If a simpler set produces equivalent behaviour, we have found a more minimal set. If no simpler set produces equivalent behaviour, our initial set was already the minimal one.

### Parameters

In this section, we will run through the different parameters the model has and what values we chose for them. Parameters control specific aspects of our model, like how many agents inhabit our virtual world or how likely reduction is. Paradoxically, choosing the right parameter values is relatively unimportant, until it is not. The consensus in simulation research is that the outcome of a simulation should not hinge on its parameter values [@stanford_revisiting_2013, @pijpops_agent-gebaseerde_2015]. For example, it usually should not matter whether 25 or 250 agents partake in a simulation[^agent-count-exception]; the emergent effect will remain the same, though it could be slower or faster. At the same time, it goes without saying that having just two or three agents in a simulation hardly makes the model world representative of an actual language "community". In the same way, while it would be hard to justify having a language of only two constructions, the difference between 100 and 1000 constructions in a modelling context is much smaller. Therefore, most of our parameter choices are practical in nature.

One of the driving forces behind our parameter choices was computational tractability. Because we work with vector representations, and interpretation hinges on vector computations on an agent's entire exemplar memory, large agent counts, high dimension counts, large vocabulary sizes and large memory sizes all cause the time it takes to run a single simulation to balloon, even on powerful server hardware. Therefore, we made the decision to keep the parameters of the simulation modest. Review @tbl-parameters for an overview of our choices.

[^agent-count-exception]: Of course, if a simulation is designed to be sensitive to agent count (i.e. because it is part of the research question), then the number of agents *does* matter.

| Parameter | Explanation | Value |
| --- | --- | --- |
| $N$ | Number of agents | 25 |
| $V$ | Number of constructions | 100 |
| $M$ | Number of vector dimensions | 100 |
| $L$ | Number of exemplars in memory | 1000 |
| $R$ | Value subtracted from exemplar vector at reduction time. Also functions as threshold floor value. A vector dimension cannot reduce below this point. | 15 |
| $n$ | Neighbourhood size. Only exemplars within this range count towards the interpretation of a vector. | 150 |
| $p$ | Reduction probability at speech time | 0.5 |
: An overview of all parameters in the simulation and their values. {#tbl-parameters}

### Implementation

We implemented our model of the reduction effect in Python using the MESA library [@python-mesa-2020]. All source code is available online.[^github-link]

[^github-link]: todo hier link

## Results

In this section, we will go over the results produced by the model operationalised according to the requirements detailed in the previous sections. More specifically, we will check if the model behaviour adheres to the expected behaviour defined in @sec-expected-behaviour.

### Base model {#sec-base-model}

First, we tested the simulation model with all preconditions we assume to be the minimal conditions for reduction (see @sec-preconditions). As a reminder, these are those minimal preconditions:

1. Language users employ a shared code
2. Language users sample constructions according to Zipf's law
3. Language users remember multiple forms of the same construction
4. Language users spontaneously reduce forms

We will discuss the behaviour of the simulation by going over the "checklist" defined in @sec-expected-behaviour.

1. The language system has reorganised itself to express the same constructions using more efficient representations.

We can see in @fig-base-model-l1-general that the mean L1 (i.e. the mean energy in the acoustic representations of constructions) of the constructicon of the agents drops sharply and then remains more or less constant. Therefore, we can say that, indeed, the system seems to have reorganised itself to be more efficient. Of course, given the non-zero probability of reduction (see @tbl-parameters), this is to be expected.

2. Frequent forms have reduced to a greater extent than non-frequent forms.

@fig-base-model-l1-per-construction shows the average L1 of each construction separately. Recall that we expect frequent forms to reduce a great extent, and faster as well. We indeed see that frequent forms (at the left of the graph) have lower L1 values than less frequent forms. In @fig-base-model-half-life-per-construction, the "half life" period for each construction is given. We show how many steps in the simulation it took for every acoustic representation to reach half of its original energy. We see that for more frequent constructions, this half time is reached faster than for less frequent constructions. Most constructions do not even show up in the graph, since they have not reached half their original energy yet.

3. Communication between agents remains successful.

For the final requirements, we refer to @fig-base-model-success. It shows both the micro-averaged and macro-averaged communicative success in the model. The micro-average refers to the total number of successes, and is influenced by Zipfian sampling. Because most of the speaking turns will concern the most frequent construction, the communicative success of that specific construction weighs significantly on the global, unadjusted success rate. The macro-average is an average across averages: each construction contributes equally towards the average, and therefore overrules Zipfian sampling. The macro-average gives a better of idea of communicative success in language system as a whole.

We see in @fig-base-model-success that overall, communicative success in the model remains quite alright. As we mentioned, it is expected that there is some trade-off between sparsity and communicative success, and the model results remain within this margin. However, the confusion matrix in @fig-base-model-matrix paints a different picture. Indeed, there seems to be mass confusion among the top 35 most frequent constructions. More specifically, all frequent constructions seem to become confused with the most frequent construction. Obviously, this is a major flaw in the communication system of the agents, and it also does not correspond with realistic linguistic behaviour: reduction effects in real life do not lead to mass confusion. 

@fig-base-model-confusion-ratio shows the "good origin ratio" for the ten most frequent constructions. The ratio expresses from 0 to 1 what share of exemplars, across agents, comes from successful interactions between agents. If an agent mistakes a heard form for a form that actually stands for another construction, the exemplar collection for the correct construction becomes diluted: another form is now also (wrongly) associated with that construction. If the exemplar collection for a specific construction contains a lot of mishearings, its good origin ratio goes down. Here specifically, we see that the most frequent form has a lower good origin ratio than the other constructions. This makes sense, given how often it is confused with other forms. The issue, however, is that we cannot know from these metrics exactly *how* the confusion arises and why the most frequent construction specifically becomes the target of the confusion. To find out the cause of confusion, we built another model, which we will discuss in the next section.

::: {#fig-base-model-group layout-ncol=2}

![Mean L1 evolution in the base model (across constructions, across agents)](figures/fig-base-model-l1-general.png){#fig-base-model-l1-general}

![Mean L1 value in the base model (per construction, across agents, t = 50,000)](figures/fig-base-model-l1-per-construction.png){#fig-base-model-l1-per-construction}

![Mean L1 half life in the base model (per construction, across agents, t = 50,000)](figures/fig-base-model-half-life-per-construction.png){#fig-base-model-half-life-per-construction}

![Global communicative success in the base model (across constructions, across agents). The micro-average is taken across all constructions. The macro-average is an average across averages, with each construction contributing equally towards the average.](figures/fig-base-model-success.png){#fig-base-model-success}

![Confusion matrix showing the mistakes that agents make in the base model. The x axis shows the intended constructions, the y axis shows the understood construction (across agents).](figures/fig-base-model-matrix.png){#fig-base-model-matrix}

![Correct interpretation ratio evolution in the base model (per construction, across agents). The ratio expresses how many times a specific construction was interpreted correctly.](figures/fig-base-model-confusion-ratio.png){#fig-base-model-confusion-ratio}

An overview of the different simulation behaviour metrics and their evolutions for the base model.
:::

### Cone model {#sec-cone-model}

The main reason why we cannot reliably investigate the cause of confusion in the base model stems from the high dimensionality of the vectors. Because we work in a 100-dimensional space, there is no straightforward way to see how the vectors evolve over time. Unfortunately, dimensionality reduction techniques obscure a large share of the variation in the vectors, and are therefore unreliable to estimate the actual course of the acoustic representations.

The solution seems simple: reducing the number of dimensions to just two, which allows you to plot the acoustic representations accurately and reliably. This is indeed the solution we opted for, but with a few adjustments. We chose to represent the acoustic space as a cone, because this brings the properties of a two-dimensional space closer to the behaviour of the multi-dimensional space: all representations have a straight path to the origin (there is no need to go "through" another representation), and the phonetic space becomes more sparse as one nears the origin. Other adjustments are mechanical adaptations to the conic shape: all exemplars now reduce by moving at a straight angle towards the origin, and all constructions start at an equal radius of 250 from the origin. We limit ourselves to just ten constructions for this demonstration.

@fig-cone-model-group shows in a straightforward way what happens to the vector space for the duration of the simulation. As the acoustic representations become sparser, the same constructions now have to compete for a smaller total acoustic space for their formal representations. As representations inch closer, their interpretations become disturbed by the presence of a more frequent neighbour. Since our reception system is sensitive to tallying, the vicinity of a more frequent neighbour causes mishearings, and as a result disrupts communicative success. We can see this process happen iteratively, until the most frequent form has overtaken the interpretation of every construction.

The reason why we do not see this iterative process reflected in the base model is again the high dimensionality of the vectors. In the cone model, there are only two dimensions in which forms can inch closer together. In the 100-dimensional model, the most frequent form can be the closest neighbours of multiple forms at once. The mechanics behind the confusion, however, are the same. A possible solution for these mishearings is given in the next section.

::: {#fig-cone-model-group layout-ncol=3}
![A 3D representation of the exemplar locations of a random agent in the cone model. Note that the vector space becomes more narrow near the top of the cone.](figures/fig-cone-model-angle-vocabulary-plot-3d-begin.png){#fig-cone-model-angle-vocabulary-plot-3d-begin}

![A flattened representation of the exemplar locations of a random agent in the cone model (t = 0).](figures/fig-cone-model-angle-vocabulary-plot-2d-begin.png){#fig-cone-model-angle-vocabulary-plot-2d-begin}

![A flattened representation of the exemplar locations of a random agent in the cone model (t = 700).](figures/fig-cone-model-angle-vocabulary-plot-2d-end.png){#fig-cone-model-angle-vocabulary-plot-2d-end}

An overview of the exemplar vector locations and their evolutions for the cone model.
:::

### Re-entrance model

Now that we can see the mishearings happen in real time, the question arises how they can be avoided. A complicating factor is that we intentionally did not incorporate a feedback mechanism in our simulation: hearers cannot know that they are mistaking a form for another, so they cannot say "come again?". Vice versa, a speaker cannot know whether a reduced form will cause a mishearing on behalf of the hearer, as they do not have access to the acoustic representations of the latter. As such, our model design without any feedback mechanism seems to lead to a linguistic catch 22.

Nevertheless, a solution within this design is possible on behalf of the speaker. Since language production is considered a complex, automatic and unconscious process, its output cannot always be guaranteed to lead to communicative success. It is in this light that @steels_language_2003 has coined the term "re-entrance". Re-entrance can be understood as the application of a speaker's reception subsystem to their own linguistic utterances, to serve as a means of incomprehension prevention or repair. The concept is related to a speaker's inner voice, and could be seen as an integral part of consciousness in itself. Re-entrance is mechanically straightforward: a speaker's intended production is routed through their own reception mechanism, and its adequacy is subsequently assessed. It is in exactly this way that we have implemented this mechanism in our simulation. If a speaker reduces an acoustic representation, we first let them reflect on whether they would understand the uttered form correctly themselves. If they would, communication can continue as planned. If they would not, reduction is reversed, and the unreduced form is communicated.

![A schematic overview of the language game played by the speaker and hearer, with re-entrance. Before speaking, the speaker checks whether they would understand the uttered form themselves. If they do, reduction is kept, but if they do not, reduction is reversed. The new elements in the overview are indicated in teal.](language_game_reentrance.svg){#fig-model_design_language_game_reentrance}

With this new precondition added to our basic assumptions, the model now behaves as expected. @fig-reentrance-model-l1-general shows an increasing sparsity, @fig-reentrance-model-l1-per-construction and @fig-reentrance-model-half-life-per-construction show reduction extent and speed to be influenced by frequency, @fig-reentrance-model-success shows adequate communicative success and @fig-reentrance-model-matrix and @fig-reentrance-model-confusion-ratio show little to no confusion. Therefore, it seems the minimal number of preconditions for the reducing effect should be increased to five:

1. Language users employ a shared code
2. Language users sample constructions according to Zipf's law
3. Language users remember multiple forms of the same construction
4. Language users spontaneously reduce forms
5. Language users should be able to understand reduced forms before uttering them

::: {#fig-reentrance-model-group layout-ncol=2}

![Mean L1 evolution in the re-entrance model (across constructions, across agents)](figures/fig-reentrance-model-l1-general.png){#fig-reentrance-model-l1-general}

![Mean L1 value in the re-entrance model (per construction, across agents, t = 50,000)](figures/fig-reentrance-model-l1-per-construction.png){#fig-reentrance-model-l1-per-construction}

![Mean L1 half life in the re-entrance model (per construction, across agents, t = 50,000)](figures/fig-reentrance-model-half-life-per-construction.png){#fig-reentrance-model-half-life-per-construction}

![Global communicative success in the re-entrance model (across constructions, across agents). The micro-average is taken across all constructions. The macro-average is an average across averages, with each construction contributing equally towards the average.](figures/fig-reentrance-model-success.png){#fig-reentrance-model-success}

![Confusion matrix showing the mistakes that agents make in the re-entrance model. The x axis shows the intended constructions, the y axis shows the understood construction (across agents).](figures/fig-reentrance-model-matrix.png){#fig-reentrance-model-matrix}

![Correct interpretation ratio evolution in the re-entrance model (per construction, across agents). The ratio expresses how many times a specific construction was interpreted correctly.](figures/fig-reentrance-model-confusion-ratio.png){#fig-reentrance-model-confusion-ratio}

An overview of the different simulation behaviour metrics and their evolutions for the re-entrance model.
:::

### Evaluation

In this section, we will evaluate our set of minimal preconditions by separately disabling them. As a reminder, these are the five minimal conditions we determined experimentally:

1. Language users employ a shared code
2. Language users sample constructions according to Zipf's law
3. Language users remember multiple forms of the same construction
4. Language users spontaneously reduce forms
5. Language users should be able to understand reduced forms before uttering them

In each of the following subsections, we will reprogram the final model to leave out one precondition. If we see equivalent behaviour (i.e. the model behaviour still adheres to the minimal expected behaviour explained in @sec-expected-behaviour), we have found a new minimal set of preconditions. If we cannot attain the expected behaviour, we know we have found the minimal set.

#### No shared code

In this model, we disable the shared code among language users. We do this by generating a completely new set of acoustic vectors for each agent separately. You could say that this means that each speaker uses a completely different language.

If we look at the results in @fig-no-shared-code-model-success, we see that there is no successful communication. Agents are unable to interpret what the other agents are saying, because they do not use the same representations to refer to the same constructions. Since communication is unsuccessful, expected behaviour #3 is violated (see @sec-expected-behaviour).

![Global communicative success in the no shared code model (across constructions, across agents). The micro-average is taken across all constructions. The macro-average is an average across averages, with each construction contributing equally towards the average.](figures/fig-no-shared-code-model-success.png){#fig-no-shared-code-model-success}

#### No Zipfian sampling

In this model, we sample constructions using a linear distribution instead of a Zipfian one. In practice, this means that the most frequent construction is no longer twice as frequent as the second most frequent construction, which is twice as frequent as the third most frequent construction, and so on. Instead, the most frequent construction is only one "unit" more frequent than the second most frequent construction, which is one "unit" more frequent than the third most frequent construction, etc. This difference is demonstrated in @fig-linear-distribution, where we show two distributions which both have 10,000 as the frequency of the most frequent construction. The frequency drop of the Zipfian distribution is much more steep than the one of the linear distribution.

```{r}
#| label: fig-linear-distribution
#| fig-cap: "Overview of the two construction distributions tested in the model" 
#| layout-ncol: 2
#| layout-nrow: 1
#| fig-subcap:
#|   - "Theoretical Zipfian distribution of constructions" 
#|   - "Theoretical Linear distribution of constructions"
#| echo: FALSE

library(ggplot2)

# Zipfian

MAX_FREQ = 10000
x <- 1:100
# Theoretical zipf
y <- x^(-1) * MAX_FREQ

# Create a data frame
data <- data.frame(x, y)

# Plot the linear distribution
ggplot(data, aes(x = x, y = y)) +
  geom_bar(stat = "identity") +
  ggtitle("Zipfean distribution") +
  xlab("Rank") +
  ylab("Frequency") +
  theme_minimal()

# Linear

x <- 1:100
y <- seq(10000, 100, -100)

# Create a data frame
data <- data.frame(x, y)

# Plot the linear distribution
ggplot(data, aes(x = x, y = y)) +
  geom_bar(stat = "identity") +
  ggtitle("Linear distribution") +
  xlab("Rank") +
  ylab("Frequency") +
  theme_minimal()
```

The question is to what extent this change from Zipfian to linear frequency has an influence on the course of the simulation, most notably on the extent of reduction among representations and their respective reduction speed. To answer this question, we look at @fig-no-zipfian-model-l1-per-construction. You can see that, generally speaking, there is hardly any difference in average L1 value among constructions. As the simulation unfolds, any variation in L1 is flattened down. In contrast, we saw in @fig-base-model-l1-per-construction (repeated below) that the Zipfian simulation maintains a clear distinction between frequent and non-frequent forms. Highly frequent forms have a lower L1 floor compared to higher frequency values, and this distinction is maintained over time.

In @fig-no-zipfian-model-half-life-per-construction, we again show the "half life" period for each construction. We see that very few acoustic representations reduce to half their energy in the Zipfian simulation, in contrast to the linear simulation, where many representations have indeed reduced at least halfway. This shows that, if frequency is assumed to be linear, reduction goes faster and applies to more forms than when frequency is assumed to be Zipfian.

In this regard, we can say that expected behaviour #2 (see @sec-expected-behaviour) is violated, since a model of reduction with assumed linear frequency does not produce any stark differences in reduction speed and extent between high and low frequency items.

::: {#fig-no-zipfian-model-l1-per-construction-group layout-ncol=2}

![Mean L1 value in the re-entrance model with Zipfian sampling (per construction, across agents, t = 50,000)](figures/fig-reentrance-model-l1-per-construction.png){#fig-base-model-l1-per-construction-repeat}

![Mean L1 value in the re-entrance model with linear sampling (per construction, across agents, t = 50,000)](figures/fig-no-zipfian-model-l1-per-construction.png){#fig-no-zipfian-model-l1-per-construction}


A comparison of the mean L1 values per construction in two models with Zipfean and linear sampling respectively.
:::

::: {#fig-no-zipfian-model-group layout-ncol=2}

![Mean L1 half life in the re-entrance model with Zipfian sampling (per construction, across agents, t = 50,000)](figures/fig-reentrance-model-half-life-per-construction.png){#fig-bases-model-half-life-per-construction-repeat}

![Mean L1 half life in the with linear sampling (per construction, across agents, t = 50,000)](figures/fig-no-zipfian-model-half-life-per-construction.png){#fig-no-zipfian-model-half-life-per-construction}

A comparison of the mean half life per construction in two models with Zipfean and linear sampling respectively.
:::

#### No remembering multiple forms

In this model, we set the memory size to 100, the same as the number of forms. In practice, this means that each construction is only represented by a single acoustic representation.

Looking at @fig-single-exemplar-model-success, we see that global communicative success is poor, while macro-averaged success is still adequate. Therefore, we can say that the most frequent forms perform poorly, while low frequency forms remain intelligible. This violates expected behaviour #3 (see @sec-expected-behaviour).

The explanation for this difference in behaviour for the different frequency types is straightforward. Because highly frequent forms have many opportunities for reduction, their representations can evolve extremely quickly. The issue is that in this way, discrepancies in the representations between different agents can arise. Therefore, two agents could have vastly different vector representations for the same construction. You can see this in the equilibrium of the global communicative success as well: it hovers around the probability set for reduction, 0.5. If an agent dares to reduce too far, communication goes amiss. Note also that the self regulation mechanism no longer works reliably with just one exemplar per construction. In contrast, less frequent forms have fewer reduction opportunities, so the probability of divergence happening here is slimmer.

We do not see the same behaviour if agents can have multiple representations for the same construction, as a larger exemplar memory "buffers" against diverging behaviour. If several representations are available, a single reduced exemplar does not cause a large sway in the overall representation of a construction, and the self regulation mechanism remains reliable as well.


::: {#fig-no-zipfian-model-group layout-ncol=2}

![Global communicative success in the model where agents do not remember multiple forms (across constructions, across agents). The micro-average is taken across all constructions. The macro-average is an average across averages, with each construction contributing equally towards the average.](figures/fig-single-exemplar-model-success.png){#fig-single-exemplar-model-success}

![Confusion matrix showing the mistakes that agents make in the model where agents do not remember multiple forms. The x axis shows the intended constructions, the y axis shows the understood construction (across agents).](figures/fig-single-exemplar-model-matrix.png){#fig-single-exemplar-model-matrix}

An overview of communicative success in the model where agents do not remember multiple forms.
:::

#### No spontaneous reduction

In this model, we set the probability of reduction to 0. This has the obvious effect that language users will no longer attempt to build more sparse representations for their acoustic representations. In @fig-no-reduction-model-l1-general, we see that the average L1 is not decreasing. This violates expected behaviour #1 (see @sec-expected-behaviour).

![Mean L1 evolution in the model without spontaneous reduction (across constructions, across agents)](figures/fig-no-reduction-model-l1-general.png){#fig-no-reduction-model-l1-general}

#### No self-regulation

The model without self-regulation is equivalent to the base model. This model is discussed in @sec-base-model. 

Each time we disabled a precondition, it led to model behaviour that did not pass our three minimum requirements. This leads us to believe that, within the scope of our implementation, our five preconditions are indeed the minimal set.

## Discussion {#sec-discussion}

In this article, we attempted to find the minimal conditions necessary to produce the reducing effect as described by @bybee_usage_2006. We found that at least the following five preconditions were necessary to produce the reducing effect with all of its properties in a simulation:

1. Language users employ a shared code
2. Language users sample constructions according to Zipf's law
3. Language users remember multiple forms of the same construction
4. Language users spontaneously reduce forms
5. Language users should be able to understand reduced forms before uttering them

In this discussion section, we will go over what the necessity of these requirements means for reduction theory, and usage-based linguistics as a whole.

### Implications for reduction theory

In our simulation, we found that there is more to the reduction principle than just the link between usage and sparsity, as evidenced by the five preconditions. The Zipfian distribution of construction frequency together with the continuous drive to spontaneously reduce forms are the driving force behind the reduction effect proper. Other preconditions, like the existence of a large memory consisting of multiple forms and required self-understanding on behalf of speakers, act as safeguards and keep the virtual linguistic system from derailing. Finally, a shared code is required to allow for communication in the first place.

Another interesting aspect of reduction concerns the way in which the acoustic representations are reorganised under the effect of reduction. In our simulation, the Zipfian frequency curve naturally makes reduction reorganise the linguistic system in the most efficient way possible: the most frequent constructions take up the most basic acoustic representations with the least amount of energy, while less frequent constructions are expressed with more elaborate acoustic representations.

To explain this reorganisation process, we start from our cone model (@sec-cone-model). At the very top of the cone, only one construction can exist, as two constructions in the same location would naturally cause confusion. The most frequent construction has the most opportunities to reduce due to its frequency, and is therefore the first construction to reach the minimal possible representation. This can be seen in the full model as well, as is clear in @fig-reentrance-model-l1-per-construction: the most frequent construction occupies the smallest possible representation: a vector filled with floor values ($M \times R = 100 \times 15 = 1500$). Less frequent constructions cannot also reduce to the smallest possible representation, so they have to stop short in their reduction path, at the expense of using slightly more energy. In the cone model, they can be found slightly below the top of the cone. Since the cone is wider here, several forms can co-exist without causing confusion. The same can be seen in @fig-reentrance-model-l1-per-construction: several constructions hover around the 2500 L1 energy level, albeit with diverse internal energy distributions to allow for a distinction between different forms. Once this phonetic space is also full, constructions will need to be expressed through forms with even more energy, which allow for even more variation, and so on. This incremental process can be identified in @fig-reentrance-model-l1-per-construction, as the energy levels can be seen as evolving into a staircase-like pattern.

It is clear that such an efficient reorganisation can only be possible with a Zipfian frequency distribution. The highly skewed frequency distribution leads to the most frequent form to use the most efficient vector position, with less frequent forms sharing a less efficient vector space. If frequency is assumed to be linear, the frequency differences between forms are not stark enough to really favour one construction over another in its reduction path. Several constructions have to compete for the same locations in the vector space, with no one construction being able to reduce completely to the most efficient location. The outcome in this scenario is that the most frequent forms collectively reach a reduction "ceiling" at a higher energy level than would be the case with a Zipfian frequency distribution.

Finally, one detail of our implementation that has not been mentioned yet, but that seems important for the reduction effect theoretically, is that of acoustic-perceptual resolution and its consequences for the avoidance of a fractal organisation of acoustic representations. Perception in our model is based on spatial vicinity. All exemplars within a specified range contribute towards the understanding of a particular form, and this range is fixed. This has the consequence that there is no use in constructions reducing to infinitely smaller forms, as an agent's perceptual range remains the same. Should our model be updated to allow for reduction in increasingly smaller steps, representations could theoretically keep reducing ad infinitum while keeping their current distinctions ("fractal"). They would just appear at increasingly smaller scales, at least as long as agents are perceptually sensitive to these small differences.[^realism] We did not include the avoidance of this fractal property as a precondition for reduction, as we considered it moreso a detail of our implementation. Still, we think this detail is theoretically interesting and could be further investigated.

[^realism]: Humans are known not to be sensitive to minute differences in acoustic frequency. In fact, an entire rescaling of the human hearing range has been devised for speech recognition purposes ["Mel scale", @pedersen_mel_1965] in order to account for the insensitivity of the human ear to certain frequency distinctions, especially in the upper frequency range.

### Implications for usage-based linguistics

As of now, most discourse about exemplars in usage-based linguistics has centred around the categorisation and structure of exemplars, their link with prototypes and their cognitive realisation. In our simulation, we see that exemplars can also act  as buffers in situations where representations undergo sudden, considerable changes. Especially for high frequency constructions, the buffer effect works well, as these constructions are assumed to have a large number of exemplars in memory simply due to their sheer frequency of occurrence. Concurrently, this buffering process is especially effective for high frequency constructions in particular, as they have the most opportunity to undergo considerable changes in the first place.

In addition, the categorisation of exemplars is shown to not be as straightforward in our simulation as is assumed theoretically. While the idea is that proximity to other exemplars should suffice for correctly categorising or understanding representations, frequency effects can disrupt this process and can cause cascading confusion. Of course, our simulation does not feature any sort of context nor feedback mechanism, which could help mitigate this categorisation problem.

### Implications for language evolution

One of the staples of research into human communication are the thirteen design principles of human communication by @hockett_origin_1960. One of these principles is 'interchangeability', i.e. the ability for speakers to utter themselves utterances that they have heard, as well as the ability for hearers to hear utterances that they have spoken themselves. In short, it is the property that affirms speakers can become hearers, and hearers can become speakers. This property is indispensible for our model of reduction, as it proved necessary for speakers to gauge how far they could reduce construction representations by testing a reduced utterance on themselves. This requirement highlights how defining principles of human communication perhaps also further define its evolution.

Re-entrance itself has been positioned by @steels_language_2003 to be essential especially for grammatical evolution. He showed that in a simulation of case marking, speakers need to be able to assess the ambiguity of their own argument structures beforehand. Based on their own evaluation, the potential necessity of case marking can be gauged. Our model does not concern grammar, but its core principle of ambiguity avoidance remains equivalent. Since our agents can only rely on vector locations to categorise an incoming representation, the ambiguity avoidance mechanism serves a similar purpose.

### Further research

There are several ways in which our model could be extended. The most obvious extension would be the inclusion of context. Indeed, while it was the very goal of our simulation to distill only the most fundamental aspects of the reduction effect, in real life situations, context contributes to how far a specific construction's representation can be reduced. If a construction is predictable from either its linguistic or its (real-life) situational context, there is a lesser need to express it fully. In this regard, it would be appropriate to turn our reduction model into a model of *probability* rather than one of frequency. TODO lezen Gregory et al.

Furthermore, our model currently does not account for the co-existence of reduced and unreduced forms. While our exemplar model allows for a wide range of variation, agents will forget unreduced forms once they fill up their exemplar memories. Other possible extensions include allowing constructions to disappear or varying the 'reducibility' of certain constructions to account for the fact that not *all* high frequency words seem eligible or reduction.

Finally, it would be valuable to bring the results of this simulation study into the real world by means of an experimental study (i.e. like @kirby_cumulative_2008). In this way, it can be investigated if and how the simulation experiment might be translated into the context of real people. In this regard, our findings *in silico* can serve as headlights to make the transition to research *in vivo* more straightforward.

## Appendix {#sec-appendix-metrics}

In order to keep track of the simulation course, we computed the following metrics at each step:

1. **Communicative success:**  
  Ratio expressing in how many speaking turns the hearer understood the speaker correctly.  
  $$
  \frac{\text{\# successful turns}}{\text{\# turns}}
  $$
4. **Mean agent L1:**  
  Number expressing the average L1 norm of exemplars, across all agents.  
  $$
  \frac{1}{N} \sum_{i=1}^{N}{\left[ \frac{1}{L_i} \sum_{j=1}^{L_i}{ \left[ \frac{\sum_{k=1}^{M}{ \text{agent}_i.\text{memory}_{jk}}}{M} \right] } \right]}
  $$
5. **Mean construction L1:**  
  Number expressing for each construction $c$ the average L1 norm of its exemplars, across all agents.
  $$  
  \frac{1}{N} \sum_{i=1}^{N}{ \left[ \frac{1}{L^c_i} \sum_{j=1}^{L^c_i}{ \left[ \frac{\sum_{k=1}^{M}{ \text{agent}_i.\text{memory}_{jk}}}{M} \right] } \right] }
  $$ with $L^c$ = # exemplars associated with construction $c$
6. **Confusion matrix:**  
  Confusion matrix tallying intended and understood constructions across all agents. Matrix of size $V \times V$.
10. **Communicative success (macro average):**  
  Ratio expressing in how many speaking turns the hearer understood the speaker correctly. Macro averaged to disable Zipfian skew.
  $$
  \frac{1}{V} \sum_{t=1}^{V}{\frac{\text{\# successful turns}}{\text{\# turns for construction } t}}
  $$
11. **Good origin ratio:**  
  Ratio expressing for each construction $c$ how many exemplars come from successful speaking turns.  
  $$
  \frac{1}{N} \sum_{i=1}^{N}{ \left[ \frac{1}{L^c_i} \sum_{j=1}^{L^c_i}{ \operatorname{good origin}(L^c_i) } \right] }
  $$ with $L^c$ = # exemplars associated with construction $c$ and $\operatorname{good origin}(exemplar) = \begin{cases}
0 & \text{if from failed speaking turn} \\
1 & \text{if from successful speaking turn}
\end{cases}$
12. **Half life time:**  
  Number expressing for each construction $c$ the timestep at which half of its original energy was reached.  
  $$
  \operatorname{min}(t) : E^c[t] \leq \frac{E^c[0]}{2}
  $$ with $t$ = time step in the model and $E^c$ list of L1 values per time step in the model for construction $c$